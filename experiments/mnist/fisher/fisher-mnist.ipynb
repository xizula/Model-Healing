{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependecies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T16:51:56.941518Z",
     "iopub.status.busy": "2025-04-03T16:51:56.941224Z",
     "iopub.status.idle": "2025-04-03T16:51:56.955583Z",
     "shell.execute_reply": "2025-04-03T16:51:56.954731Z",
     "shell.execute_reply.started": "2025-04-03T16:51:56.941486Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import copy\n",
    "from pathlib import Path\n",
    "from torchvision import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T16:51:56.957966Z",
     "iopub.status.busy": "2025-04-03T16:51:56.957322Z",
     "iopub.status.idle": "2025-04-03T16:51:56.971856Z",
     "shell.execute_reply": "2025-04-03T16:51:56.970817Z",
     "shell.execute_reply.started": "2025-04-03T16:51:56.957920Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "project_root = Path.cwd().resolve().parents[2]\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.append(str(project_root))\n",
    "\n",
    "data_root = project_root / \"data\"\n",
    "data_root.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "from notebook_setup import setup_notebook\n",
    "\n",
    "setup_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T16:51:56.973096Z",
     "iopub.status.busy": "2025-04-03T16:51:56.972801Z",
     "iopub.status.idle": "2025-04-03T16:51:56.988678Z",
     "shell.execute_reply": "2025-04-03T16:51:56.987826Z",
     "shell.execute_reply.started": "2025-04-03T16:51:56.973068Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "from utils.utils import DEVICE\n",
    "\n",
    "print(f\"Device used: {DEVICE}\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "from utils.utils import set_seed\n",
    "\n",
    "set_seed()\n",
    "\n",
    "from utils.utils import save_model\n",
    "\n",
    "from models.simple_cnn import load_model_cnn, init_model_cnn\n",
    "\n",
    "# Merics\n",
    "from utils.train_test_metrics import test_model, show_metrics\n",
    "\n",
    "# Recreate Dataloaders from json files\n",
    "from methods.naive.naive_utils import recreate_dataloaders\n",
    "\n",
    "# Fisher Information Matrix (FIM) calc and unlearning with FIM\n",
    "from methods.fisher.fisher_utils import (\n",
    "    iterative_fisher_unlearn,\n",
    "    create_unlearning_dataloader,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters (arbitrary chosen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T16:51:56.990870Z",
     "iopub.status.busy": "2025-04-03T16:51:56.990564Z",
     "iopub.status.idle": "2025-04-03T16:51:57.000762Z",
     "shell.execute_reply": "2025-04-03T16:51:56.999888Z",
     "shell.execute_reply.started": "2025-04-03T16:51:56.990845Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 2048\n",
    "MINI_BATCH_SIZE = 4096\n",
    "\n",
    "SIGMA = 0.5\n",
    "\n",
    "EPS = 1e-2\n",
    "MAX_NORM = 0.5\n",
    "DEEP_CLEAN_THRESHOLD = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CALL FISHER\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T16:51:57.002049Z",
     "iopub.status.busy": "2025-04-03T16:51:57.001761Z",
     "iopub.status.idle": "2025-04-03T16:51:57.012968Z",
     "shell.execute_reply": "2025-04-03T16:51:57.012367Z",
     "shell.execute_reply.started": "2025-04-03T16:51:57.002021Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model_file = project_root / \"experiments/mnist/naive/CNN_MNIST_model.pth\"\n",
    "samples_to_unlearn_file = (\n",
    "    project_root / \"experiments/mnist/naive/mnist_samples_to_unlearn_30per.json\"\n",
    ")\n",
    "remaining_dataset_file = (\n",
    "    project_root / \"experiments/mnist/naive/updated_mnist_data_splits.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Init models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T16:51:57.014172Z",
     "iopub.status.busy": "2025-04-03T16:51:57.013899Z",
     "iopub.status.idle": "2025-04-03T16:51:57.049079Z",
     "shell.execute_reply": "2025-04-03T16:51:57.048110Z",
     "shell.execute_reply.started": "2025-04-03T16:51:57.014145Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "original_model, original_model_name, criterion, _optimizer, transform = load_model_cnn(\n",
    "    model_pth_path=model_file\n",
    ")\n",
    "\n",
    "model_to_unlearn = copy.deepcopy(original_model)\n",
    "import torch.nn as nn\n",
    "\n",
    "model_to_unlearn = nn.DataParallel(model_to_unlearn, device_ids=[0, 1])\n",
    "\n",
    "model_to_unlearn_name = \"fisher_\" + original_model_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T16:51:57.050975Z",
     "iopub.status.busy": "2025-04-03T16:51:57.050350Z",
     "iopub.status.idle": "2025-04-03T16:51:57.136799Z",
     "shell.execute_reply": "2025-04-03T16:51:57.135973Z",
     "shell.execute_reply.started": "2025-04-03T16:51:57.050934Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_dataset = datasets.MNIST(\n",
    "    root=data_root, train=True, transform=transform, download=True\n",
    ")\n",
    "test_dataset = datasets.MNIST(\n",
    "    root=data_root, train=False, transform=transform, download=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T16:51:57.138336Z",
     "iopub.status.busy": "2025-04-03T16:51:57.137933Z",
     "iopub.status.idle": "2025-04-03T16:51:57.158611Z",
     "shell.execute_reply": "2025-04-03T16:51:57.157942Z",
     "shell.execute_reply.started": "2025-04-03T16:51:57.138293Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "unlearn_indices, _unlearn_loader = create_unlearning_dataloader(\n",
    "    samples_to_unlearn_file, train_dataset, batch_size=MINI_BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T16:51:57.159831Z",
     "iopub.status.busy": "2025-04-03T16:51:57.159510Z",
     "iopub.status.idle": "2025-04-03T16:51:57.163752Z",
     "shell.execute_reply": "2025-04-03T16:51:57.162869Z",
     "shell.execute_reply.started": "2025-04-03T16:51:57.159804Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# train_loader, _val_loader, test_loader, classes = recreate_dataloaders(\n",
    "#     data_splits_file=remaining_dataset_file,\n",
    "#     datasets=(train_dataset, test_dataset),\n",
    "#     batch_size=BATCH_SIZE)\n",
    "\n",
    "# unlearn_loader = create_unlearning_dataloader(samples_to_unlearn_file, train_dataset, batch_size = MINI_BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T16:51:57.166653Z",
     "iopub.status.busy": "2025-04-03T16:51:57.166333Z",
     "iopub.status.idle": "2025-04-03T16:51:57.195298Z",
     "shell.execute_reply": "2025-04-03T16:51:57.194469Z",
     "shell.execute_reply.started": "2025-04-03T16:51:57.166621Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import json\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "from utils.utils import DEVICE\n",
    "\n",
    "\n",
    "def compute_gradient_on_subset(model, criterion, dataset_subset, batch_size):\n",
    "    \"\"\"\n",
    "    Compute the average gradient Δ_rem = ∇L(θ, D') over the given dataset_subset.\n",
    "    \"\"\"\n",
    "    dataloader = DataLoader(dataset_subset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    grad_dict = {}\n",
    "    total_samples = 0\n",
    "\n",
    "    model.train()\n",
    "    for inputs, targets in tqdm(dataloader, desc=\"Computing gradients\"):\n",
    "        inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
    "        model.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        current_batch = inputs.size(0)\n",
    "        total_samples += current_batch\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.requires_grad and param.grad is not None:\n",
    "                if name not in grad_dict:\n",
    "                    grad_dict[name] = param.grad.detach().clone() * current_batch\n",
    "                else:\n",
    "                    grad_dict[name] += param.grad.detach() * current_batch\n",
    "\n",
    "    # Average gradients over the entire subset\n",
    "    for name in grad_dict:\n",
    "        grad_dict[name] /= total_samples\n",
    "\n",
    "    return grad_dict\n",
    "\n",
    "\n",
    "def compute_fisher_on_subset(model, criterion, dataset_subset, batch_size):\n",
    "    \"\"\"\n",
    "    Compute a diagonal approximation of the Fisher Information Matrix F over the given dataset_subset.\n",
    "    It averages the squared gradients.\n",
    "    \"\"\"\n",
    "    dataloader = DataLoader(dataset_subset, batch_size=batch_size, shuffle=False)\n",
    "    fisher_diag = {}\n",
    "    total_samples = 0\n",
    "\n",
    "    model.eval()\n",
    "    for inputs, targets in tqdm(dataloader, desc=\"Computing Fisher\"):\n",
    "        inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
    "        model.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        current_batch = inputs.size(0)\n",
    "        total_samples += current_batch\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.requires_grad and param.grad is not None:\n",
    "                if name not in fisher_diag:\n",
    "                    fisher_diag[name] = (param.grad.detach() ** 2) * current_batch\n",
    "                else:\n",
    "                    fisher_diag[name] += (param.grad.detach() ** 2) * current_batch\n",
    "\n",
    "    for name in fisher_diag:\n",
    "        fisher_diag[name] /= total_samples\n",
    "\n",
    "    return fisher_diag\n",
    "\n",
    "\n",
    "def remove_from_fisher_incrementally(\n",
    "    fisher_diag, model, criterion, dataset_removed, batch_size\n",
    "):\n",
    "    dataloader = DataLoader(dataset_removed, batch_size=batch_size, shuffle=False)\n",
    "    total_removed_samples = 0\n",
    "\n",
    "    model.eval()\n",
    "    for inputs, targets in dataloader:\n",
    "        inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
    "        model.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        batch_samples = inputs.size(0)\n",
    "        total_removed_samples += batch_samples\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.requires_grad and param.grad is not None:\n",
    "                fisher_diag[name] -= (param.grad.detach() ** 2) * batch_samples\n",
    "\n",
    "    total_samples_remaining = fisher_diag[\"_total_samples\"] - total_removed_samples\n",
    "    for name in fisher_diag:\n",
    "        if name != \"_total_samples\":\n",
    "            fisher_diag[name] = torch.clamp(fisher_diag[name], min=1e-8)\n",
    "            fisher_diag[name] /= total_samples_remaining\n",
    "    fisher_diag[\"_total_samples\"] = total_samples_remaining\n",
    "\n",
    "    return fisher_diag\n",
    "\n",
    "\n",
    "def iterative_fisher_unlearn(\n",
    "    model,\n",
    "    criterion,\n",
    "    full_dataset,\n",
    "    removal_indices,\n",
    "    sigma,\n",
    "    deletion_batch_size,\n",
    "    compute_batch_size,\n",
    "    eps,\n",
    "    max_norm,\n",
    "):\n",
    "    \"\"\"\n",
    "    Implements the iterative Fisher unlearning procedure following theory:\n",
    "\n",
    "    Inputs:\n",
    "      - model: a pretrained PyTorch model (trained on full dataset D).\n",
    "      - criterion: loss function (e.g., CrossEntropyLoss).\n",
    "      - full_dataset: the full training dataset D (e.g., MNIST training set).\n",
    "      - removal_indices: list of indices (from the JSON file) to be deleted (Dₘ). E.g., 6000 samples.\n",
    "      - sigma: noise parameter σ.\n",
    "      - deletion_batch_size: desired mini-batch size for deletion (m′). E.g., 1000.\n",
    "      - compute_batch_size: batch size used when computing gradients/Fisher (BATCH_SIZE).\n",
    "      - eps: for numerical stability\n",
    "\n",
    "    Procedure:\n",
    "      1. Let current_indices = set(range(len(full_dataset))).\n",
    "      2. Partition removal_indices into mini-batches of size deletion_batch_size.\n",
    "      3. For each mini-batch, update current_indices by removing those indices.\n",
    "      4. Create a Subset from full_dataset using current_indices (this is D').\n",
    "      5. Compute Δ_rem and diagonal Fisher F on D' and update model:\n",
    "             θ ← θ − F⁻¹ Δ_rem + σ · F^(–1/4) · ε.\n",
    "    \"\"\"\n",
    "    full_size = len(full_dataset)\n",
    "    current_indices = set(range(full_size))\n",
    "\n",
    "    # Partition removal_indices into mini-batches, where s = m /m'\n",
    "    removal_list = list(removal_indices)\n",
    "    num_batches = math.ceil(len(removal_list) / deletion_batch_size)\n",
    "    partitioned_removals = [\n",
    "        removal_list[i * deletion_batch_size : (i + 1) * deletion_batch_size]\n",
    "        for i in range(num_batches)\n",
    "    ]\n",
    "    print(\n",
    "        f\"Total deletion samples: {len(removal_list)}; partitioned into {num_batches} mini-batches (each up to {deletion_batch_size} samples).\"\n",
    "    )\n",
    "\n",
    "    # Iterate over each deletion mini-batch\n",
    "    for i, batch in enumerate(\n",
    "        tqdm(partitioned_removals, desc=\"Fisher step over mini-batches\")\n",
    "    ):\n",
    "        # Remove the current batch of indices from current_indices\n",
    "        current_indices -= set(batch)\n",
    "        updated_indices = sorted(list(current_indices))\n",
    "        # Create a Subset corresponding to the updated dataset D' = D \\ (deleted so far)\n",
    "        dataset_remaining = Subset(full_dataset, updated_indices)\n",
    "        print(\n",
    "            f\"Iteration {i+1}/{num_batches}: Remaining dataset size = {len(dataset_remaining)}\"\n",
    "        )\n",
    "\n",
    "        # Compute the average gradient and diagonal Fisher on D'\n",
    "        grad_dict = compute_gradient_on_subset(\n",
    "            model, criterion, dataset_remaining, compute_batch_size\n",
    "        )\n",
    "        fisher_diag = compute_fisher_on_subset(\n",
    "            model, criterion, dataset_remaining, compute_batch_size\n",
    "        )\n",
    "\n",
    "        # Update model parameters using the Newton correction and noise injection\n",
    "        with torch.no_grad():\n",
    "            for name in grad_dict:\n",
    "                grad = grad_dict[name]\n",
    "                norm = grad.norm(2).item()\n",
    "                grad_min = grad.min().item()\n",
    "                grad_max = grad.max().item()\n",
    "                grad_mean = grad.mean().item()\n",
    "                grad_std = grad.std().item()\n",
    "                print(\n",
    "                    f\"[Raw] Param {name}: norm = {norm:.4e}, min = {grad_min:.4e}, max = {grad_max:.4e}, mean = {grad_mean:.4e}, std = {grad_std:.4e}\"\n",
    "                )\n",
    "\n",
    "            # First, compute and clip gradients, and monitor norms\n",
    "            total_grad_norm_before = 0.0\n",
    "            total_grad_norm_after = 0.0\n",
    "            for name in grad_dict:\n",
    "                norm_before = grad_dict[name].norm(2)\n",
    "                total_grad_norm_before += norm_before.item()\n",
    "                if norm_before > max_norm:\n",
    "                    grad_dict[name] = grad_dict[name] * (max_norm / norm_before)\n",
    "                norm_after = grad_dict[name].norm(2)\n",
    "                total_grad_norm_after += norm_after.item()\n",
    "\n",
    "            print(\n",
    "                f\"Iteration {i+1}: Total gradient norm before clipping = {total_grad_norm_before:.4f}\"\n",
    "            )\n",
    "            print(\n",
    "                f\"Iteration {i+1}: Total gradient norm after clipping  = {total_grad_norm_after:.4f}\"\n",
    "            )\n",
    "\n",
    "            # Now, update model parameters using the clipped gradients and monitor the Newton update norm\n",
    "            total_update_norm = 0.0\n",
    "            for name, param in model.named_parameters():\n",
    "                if param.requires_grad:\n",
    "\n",
    "                    inv_fisher = (fisher_diag[name] + eps).pow(-1)\n",
    "                    newton_update = inv_fisher * grad_dict[name]\n",
    "                    total_update_norm += newton_update.norm(2).item()\n",
    "                    param.data = param.data - newton_update\n",
    "\n",
    "                    inv_fisher_quarter = (fisher_diag[name] + eps).pow(-0.25)\n",
    "                    noise = torch.randn_like(param.data)\n",
    "                    param.data = param.data + sigma * inv_fisher_quarter * noise\n",
    "\n",
    "            print(\n",
    "                f\"Iteration {i+1}: Total Newton update norm = {total_update_norm:.4f}\"\n",
    "            )\n",
    "        print(f\"Iteration {i+1}/{num_batches} update completed.\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T16:51:57.196687Z",
     "iopub.status.busy": "2025-04-03T16:51:57.196415Z",
     "iopub.status.idle": "2025-04-03T16:53:29.719291Z",
     "shell.execute_reply": "2025-04-03T16:53:29.718398Z",
     "shell.execute_reply.started": "2025-04-03T16:51:57.196663Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.perf_counter()\n",
    "\n",
    "unlearned_model = iterative_fisher_unlearn(\n",
    "    model_to_unlearn,\n",
    "    criterion,\n",
    "    train_dataset,\n",
    "    unlearn_indices,\n",
    "    SIGMA,\n",
    "    deletion_batch_size=MINI_BATCH_SIZE,\n",
    "    compute_batch_size=BATCH_SIZE,\n",
    "    eps=EPS,\n",
    "    max_norm=MAX_NORM,\n",
    ")\n",
    "\n",
    "end_time = time.perf_counter()  # End timer\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "print(f\"Execution time: {elapsed_time:.6f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T16:54:18.052669Z",
     "iopub.status.busy": "2025-04-03T16:54:18.052291Z",
     "iopub.status.idle": "2025-04-03T16:54:18.062113Z",
     "shell.execute_reply": "2025-04-03T16:54:18.061200Z",
     "shell.execute_reply.started": "2025-04-03T16:54:18.052639Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "save_model(unlearned_model, f\"{model_to_unlearn_name}_model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test and metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T16:54:19.534910Z",
     "iopub.status.busy": "2025-04-03T16:54:19.534202Z",
     "iopub.status.idle": "2025-04-03T16:54:19.539164Z",
     "shell.execute_reply": "2025-04-03T16:54:19.538176Z",
     "shell.execute_reply.started": "2025-04-03T16:54:19.534876Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "classes = test_dataset.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T16:54:21.344989Z",
     "iopub.status.busy": "2025-04-03T16:54:21.344618Z",
     "iopub.status.idle": "2025-04-03T16:54:23.212685Z",
     "shell.execute_reply": "2025-04-03T16:54:23.211829Z",
     "shell.execute_reply.started": "2025-04-03T16:54:21.344959Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model, *_ = init_model_cnn()\n",
    "model = nn.DataParallel(model, device_ids=[0, 1])\n",
    "model_path = f\"{model_to_unlearn_name}_model.pth\"\n",
    "test_model(model, model_to_unlearn_name, model_path, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T16:54:25.151634Z",
     "iopub.status.busy": "2025-04-03T16:54:25.151263Z",
     "iopub.status.idle": "2025-04-03T16:54:25.663056Z",
     "shell.execute_reply": "2025-04-03T16:54:25.662262Z",
     "shell.execute_reply.started": "2025-04-03T16:54:25.151602Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "predictions_path = f\"{model_to_unlearn_name}_predictions.json\"\n",
    "# classes = ['0 - zero', '1 - one', '2 - two', '3 - three', '4 - four', '5 - five', '6 - six', '7 - seven', '8 - eight', '9 - nine']\n",
    "show_metrics(predictions_path, classes, model_to_unlearn_name)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "isSourceIdPinned": true,
     "modelId": 177544,
     "modelInstanceId": 155068,
     "sourceId": 181928,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
