{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependecies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T17:33:16.056669Z",
     "iopub.status.busy": "2025-05-06T17:33:16.056075Z",
     "iopub.status.idle": "2025-05-06T17:33:20.671018Z",
     "shell.execute_reply": "2025-05-06T17:33:20.670195Z",
     "shell.execute_reply.started": "2025-05-06T17:33:16.056632Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "import json\n",
    "from tqdm import tqdm, trange\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, Subset, ConcatDataset\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T17:33:20.672522Z",
     "iopub.status.busy": "2025-05-06T17:33:20.672185Z",
     "iopub.status.idle": "2025-05-06T17:33:20.679451Z",
     "shell.execute_reply": "2025-05-06T17:33:20.678564Z",
     "shell.execute_reply.started": "2025-05-06T17:33:20.672503Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "project_root = Path.cwd().resolve().parents[2]\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.append(str(project_root))\n",
    "\n",
    "data_root = project_root / \"data\"\n",
    "data_root.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "from notebook_setup import setup_notebook\n",
    "\n",
    "setup_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T17:33:22.792634Z",
     "iopub.status.busy": "2025-05-06T17:33:22.792369Z",
     "iopub.status.idle": "2025-05-06T17:33:24.147355Z",
     "shell.execute_reply": "2025-05-06T17:33:24.146789Z",
     "shell.execute_reply.started": "2025-05-06T17:33:22.792613Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "from utils.utils import DEVICE\n",
    "\n",
    "print(f\"Device used: {DEVICE}\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "from utils.utils import set_seed\n",
    "\n",
    "set_seed()\n",
    "\n",
    "from utils.utils import save_model\n",
    "\n",
    "# Prepare Dataloaders\n",
    "from methods.naive.naive_utils import init_dataloaders\n",
    "\n",
    "# Train loop\n",
    "from utils.train_test_metrics import train_model\n",
    "\n",
    "# Plot losses\n",
    "from utils.train_test_metrics import plot_training_history\n",
    "\n",
    "# Test function\n",
    "from utils.train_test_metrics import test_model\n",
    "\n",
    "# Merics\n",
    "from utils.train_test_metrics import show_metrics\n",
    "\n",
    "# Init model\n",
    "from models.simple_cnn import init_model_cnn, load_model_cnn\n",
    "\n",
    "from methods.fisher.fisher_utils import iterative_fisher_unlearn\n",
    "\n",
    "# from methods.influence.influence_utils import iterative_influence_unlearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters (arbitrary chosen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T17:33:25.159398Z",
     "iopub.status.busy": "2025-05-06T17:33:25.158688Z",
     "iopub.status.idle": "2025-05-06T17:33:25.163001Z",
     "shell.execute_reply": "2025-05-06T17:33:25.162382Z",
     "shell.execute_reply.started": "2025-05-06T17:33:25.159373Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "LEARNING_RATE = 0.001\n",
    "EPOCHS = 10\n",
    "NUM_UNLEARN_SAMPLES_LOW_LOGIT = 25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Representative & Backup Selection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T17:33:27.278285Z",
     "iopub.status.busy": "2025-05-06T17:33:27.277665Z",
     "iopub.status.idle": "2025-05-06T17:33:27.282059Z",
     "shell.execute_reply": "2025-05-06T17:33:27.281415Z",
     "shell.execute_reply.started": "2025-05-06T17:33:27.278261Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def select_representative_and_backup_indices(dataset, rep_size=25000):\n",
    "    \"\"\"\n",
    "    Randomly select rep_size indices from the dataset as the representative subset,\n",
    "    and treat the remaining indices as the backup set.\n",
    "    \"\"\"\n",
    "    total_indices = list(range(len(dataset)))\n",
    "    rep_indices = random.sample(total_indices, rep_size)\n",
    "    backup_indices = list(set(total_indices) - set(rep_indices))\n",
    "    return rep_indices, backup_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unlearn Sample Selection (for target class)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T17:33:28.908791Z",
     "iopub.status.busy": "2025-05-06T17:33:28.908508Z",
     "iopub.status.idle": "2025-05-06T17:33:28.913337Z",
     "shell.execute_reply": "2025-05-06T17:33:28.912583Z",
     "shell.execute_reply.started": "2025-05-06T17:33:28.908771Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class IndexedDataset(Dataset):\n",
    "    def __init__(self, base_dataset, indices):\n",
    "        self.base = base_dataset\n",
    "        self.indices = indices\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        orig_idx = self.indices[i]\n",
    "        img, _ = self.base[orig_idx]\n",
    "        return orig_idx, img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T17:33:30.413038Z",
     "iopub.status.busy": "2025-05-06T17:33:30.412705Z",
     "iopub.status.idle": "2025-05-06T17:33:30.417189Z",
     "shell.execute_reply": "2025-05-06T17:33:30.416345Z",
     "shell.execute_reply.started": "2025-05-06T17:33:30.413006Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T17:33:32.090183Z",
     "iopub.status.busy": "2025-05-06T17:33:32.089534Z",
     "iopub.status.idle": "2025-05-06T17:33:32.095007Z",
     "shell.execute_reply": "2025-05-06T17:33:32.094122Z",
     "shell.execute_reply.started": "2025-05-06T17:33:32.090147Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class IndexedDatasetWithLabel(Dataset):\n",
    "    def __init__(self, base_dataset, indices_list):\n",
    "        self.base_dataset = base_dataset\n",
    "        self.indices_list = indices_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices_list)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        original_idx = self.indices_list[i]\n",
    "        img, label = self.base_dataset[original_idx]\n",
    "        return original_idx, img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T17:33:33.724741Z",
     "iopub.status.busy": "2025-05-06T17:33:33.724467Z",
     "iopub.status.idle": "2025-05-06T17:33:33.732779Z",
     "shell.execute_reply": "2025-05-06T17:33:33.731877Z",
     "shell.execute_reply.started": "2025-05-06T17:33:33.724720Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_lowest_logit_samples(\n",
    "    model, full_dataset, subset_indices, num_samples, device, batch_size=128\n",
    "):\n",
    "    \"\"\"\n",
    "    Identifies samples from a specified subset of the full_dataset that have\n",
    "    the lowest logit values for their true class.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The trained model.\n",
    "        full_dataset (Dataset): The complete dataset (e.g., train_dataset_full).\n",
    "        subset_indices (list[int]): List of indices from full_dataset that define\n",
    "                                    the subset to evaluate (e.g., rep_indices).\n",
    "        num_samples (int): The number of samples with the lowest true-class logits to return.\n",
    "        device (str): The device to run the model on ('cuda' or 'cpu').\n",
    "        batch_size (int): Batch size for processing.\n",
    "\n",
    "    Returns:\n",
    "        list[int]: A list of `num_samples` original indices (from full_dataset)\n",
    "                   corresponding to the samples with the lowest true-class logits.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    eval_dataset = IndexedDatasetWithLabel(full_dataset, subset_indices)\n",
    "    eval_loader = DataLoader(\n",
    "        eval_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=4,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    logit_infos = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for original_indices_batch, inputs_batch, labels_batch in tqdm(\n",
    "            eval_loader, desc=\"Getting logits for true classes\"\n",
    "        ):\n",
    "            inputs_batch = inputs_batch.to(device)\n",
    "            labels_batch = labels_batch.to(device)\n",
    "\n",
    "            outputs_batch = model(inputs_batch)\n",
    "\n",
    "            true_class_logits_batch = outputs_batch.gather(\n",
    "                1, labels_batch.view(-1, 1)\n",
    "            ).squeeze()\n",
    "\n",
    "            for i in range(len(original_indices_batch)):\n",
    "                logit_infos.append(\n",
    "                    (\n",
    "                        true_class_logits_batch[i].item(),\n",
    "                        original_indices_batch[i].item(),\n",
    "                        labels_batch[i].item(),\n",
    "                    )\n",
    "                )\n",
    "\n",
    "    logit_infos.sort(key=lambda x: x[0])\n",
    "\n",
    "    # Get the original indices of the num_samples with the lowest logits\n",
    "    lowest_logit_indices = [info[1] for info in logit_infos[:num_samples]]\n",
    "\n",
    "    print(\n",
    "        f\"Identified {len(lowest_logit_indices)} samples with lowest true-class logits.\"\n",
    "    )\n",
    "    print(\"Top samples with lowest true-class logits:\")\n",
    "    for i in range(min(num_samples, 5, len(logit_infos))):\n",
    "        logit, orig_idx, class_lbl = logit_infos[i]\n",
    "        print(\n",
    "            f\"  Original Index: {orig_idx}, Class: {class_lbl}, True-Class Logit: {logit:.4f}\"\n",
    "        )\n",
    "\n",
    "    return lowest_logit_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T17:33:37.698890Z",
     "iopub.status.busy": "2025-05-06T17:33:37.698403Z",
     "iopub.status.idle": "2025-05-06T17:33:37.709977Z",
     "shell.execute_reply": "2025-05-06T17:33:37.709266Z",
     "shell.execute_reply.started": "2025-05-06T17:33:37.698866Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def find_class_constrained_shadow_twins_batched_per_class(\n",
    "    twin_finder_func,\n",
    "    model,\n",
    "    full_dataset,\n",
    "    all_backup_indices,\n",
    "    unlearn_original_indices,\n",
    "    score_type,\n",
    "    batch_size,\n",
    "    device,\n",
    "    **kwargs,\n",
    "):\n",
    "    \"\"\"\n",
    "    Finds unique shadow twins for unlearn samples, constrained by class.\n",
    "    A twin for an unlearn sample of class C will only be sought from backup samples of class C.\n",
    "    Processes unlearn samples batched by class.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (overall_mapping, overall_similarity)\n",
    "               overall_mapping (dict): unlearn_idx -> backup_idx\n",
    "               overall_similarity (dict): unlearn_idx -> similarity_score\n",
    "    \"\"\"\n",
    "    overall_mapping = {}\n",
    "    overall_similarity = {}\n",
    "\n",
    "    unlearn_by_class = defaultdict(list)\n",
    "    for idx in unlearn_original_indices:\n",
    "        _, class_label = full_dataset[idx]\n",
    "        unlearn_by_class[class_label].append(idx)\n",
    "\n",
    "    backup_by_class = defaultdict(list)\n",
    "    for idx in all_backup_indices:\n",
    "        _, class_label = full_dataset[idx]\n",
    "        backup_by_class[class_label].append(idx)\n",
    "\n",
    "    used_backup_indices_globally = set()\n",
    "\n",
    "    print(\n",
    "        f\"Finding class-constrained twins for {len(unlearn_original_indices)} samples, grouped into {len(unlearn_by_class)} classes.\"\n",
    "    )\n",
    "\n",
    "    for class_label, class_unlearn_indices in tqdm(\n",
    "        unlearn_by_class.items(), desc=\"Processing classes for twin finding\"\n",
    "    ):\n",
    "        if not class_unlearn_indices:\n",
    "            continue\n",
    "\n",
    "        class_specific_backup_candidates = [\n",
    "            b_idx\n",
    "            for b_idx in backup_by_class.get(class_label, [])\n",
    "            if b_idx not in used_backup_indices_globally\n",
    "        ]\n",
    "\n",
    "        if not class_specific_backup_candidates:\n",
    "            print(\n",
    "                f\"Warning: No available backup samples of class {class_label} for {len(class_unlearn_indices)} unlearn samples.\"\n",
    "            )\n",
    "            for ui in class_unlearn_indices:\n",
    "                overall_mapping[ui] = -1\n",
    "                overall_similarity[ui] = (\n",
    "                    float(\"inf\")\n",
    "                    if score_type in [\"l2\", \"mahalanobis\"]\n",
    "                    else float(\"-inf\")\n",
    "                )\n",
    "            continue\n",
    "\n",
    "        # print(f\"  Class {class_label}: {len(class_unlearn_indices)} unlearn samples, {len(class_specific_backup_candidates)} backup candidates.\")\n",
    "\n",
    "        # Call the underlying twin finder function\n",
    "        finder_args = {\n",
    "            \"dataset\": full_dataset,\n",
    "            \"backup_indices\": class_specific_backup_candidates,\n",
    "            \"unlearn_indices\": class_unlearn_indices,\n",
    "            \"score_type\": score_type,\n",
    "            \"batch_size\": batch_size,\n",
    "            \"device\": device,\n",
    "        }\n",
    "        if twin_finder_func == find_unique_shadow_twins_features:\n",
    "            finder_args[\"model\"] = model\n",
    "\n",
    "        finder_args.update(kwargs)\n",
    "\n",
    "        try:\n",
    "            current_mapping, current_similarity = twin_finder_func(**finder_args)\n",
    "        except Exception as e:\n",
    "            print(f\"Error in twin_finder_func for class {class_label}: {e}\")\n",
    "            for ui in class_unlearn_indices:\n",
    "                overall_mapping[ui] = -1\n",
    "                overall_similarity[ui] = (\n",
    "                    float(\"inf\")\n",
    "                    if score_type in [\"l2\", \"mahalanobis\"]\n",
    "                    else float(\"-inf\")\n",
    "                )\n",
    "            continue\n",
    "\n",
    "        for u_idx, b_idx in current_mapping.items():\n",
    "            if b_idx in used_backup_indices_globally:\n",
    "                print(\n",
    "                    f\"Warning: Backup index {b_idx} for unlearn {u_idx} (class {class_label}) was already used globally or assigned multiple times in this class call. Prioritizing first assignment or skipping.\"\n",
    "                )\n",
    "            if u_idx not in overall_mapping or overall_mapping[u_idx] == -1:\n",
    "                overall_mapping[u_idx] = -1\n",
    "                overall_similarity[u_idx] = (\n",
    "                    float(\"inf\")\n",
    "                    if score_type in [\"l2\", \"mahalanobis\"]\n",
    "                    else float(\"-inf\")\n",
    "                )\n",
    "            else:\n",
    "                overall_mapping[u_idx] = b_idx\n",
    "                overall_similarity[u_idx] = current_similarity[u_idx]\n",
    "                used_backup_indices_globally.add(b_idx)\n",
    "\n",
    "        # Ensure all unlearn samples for this class are handled, even if twin_finder_func returned empty\n",
    "        for u_idx in class_unlearn_indices:\n",
    "            if u_idx not in overall_mapping:\n",
    "                overall_mapping[u_idx] = -1\n",
    "                overall_similarity[u_idx] = (\n",
    "                    float(\"inf\")\n",
    "                    if score_type in [\"l2\", \"mahalanobis\"]\n",
    "                    else float(\"-inf\")\n",
    "                )\n",
    "\n",
    "    # Filter out entries where no twin was found\n",
    "    final_mapping = {k: v for k, v in overall_mapping.items() if v != -1}\n",
    "    final_similarity = {\n",
    "        k: v for k, v in overall_similarity.items() if k in final_mapping\n",
    "    }\n",
    "\n",
    "    if len(final_mapping) < len(unlearn_original_indices):\n",
    "        print(\n",
    "            f\"Warning: Only found twins for {len(final_mapping)} out of {len(unlearn_original_indices)} unlearn samples due to class constraints or lack of available unique backups.\"\n",
    "        )\n",
    "\n",
    "    return final_mapping, final_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T17:33:40.913694Z",
     "iopub.status.busy": "2025-05-06T17:33:40.913185Z",
     "iopub.status.idle": "2025-05-06T17:33:40.929942Z",
     "shell.execute_reply": "2025-05-06T17:33:40.928917Z",
     "shell.execute_reply.started": "2025-05-06T17:33:40.913658Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def find_unique_shadow_twins_raw(\n",
    "    dataset,\n",
    "    backup_indices,\n",
    "    unlearn_indices,\n",
    "    score_type=\"l2\",\n",
    "    batch_size=256,\n",
    "    device=\"cpu\",\n",
    "):\n",
    "    \"\"\"\n",
    "    For each unlearn sample (specified by its original index in the full dataset),\n",
    "    find a unique shadow twin from backup_indices based on a similarity metric computed on raw pixel vectors.\n",
    "\n",
    "    Parameters:\n",
    "        dataset (Dataset): The full dataset, where dataset[idx][0] returns an image tensor.\n",
    "        backup_indices (list[int]): Indices for candidate backup samples.\n",
    "        unlearn_indices (list[int]): Indices for samples to be unlearned.\n",
    "        score_type (str): The metric to use: \"l2\" for Euclidean distance or \"mahalanobis\" for Mahalanobis distance.\n",
    "\n",
    "    Returns:\n",
    "        mapping (dict): A mapping from each unlearn sample original index to its unique backup index.\n",
    "        similarity_metrics (dict): A mapping from each unlearn sample original index to its computed distance.\n",
    "                                  (Lower values indicate higher similarity.)\n",
    "    \"\"\"\n",
    "\n",
    "    # Helper: load & flatten a list of indices into a single [N × D] tensor\n",
    "    def load_flat(idxs, to_device=None):\n",
    "        ds = IndexedDataset(dataset, idxs)\n",
    "        loader = DataLoader(\n",
    "            ds, batch_size=batch_size, num_workers=4, shuffle=False, pin_memory=True\n",
    "        )\n",
    "        flats = []\n",
    "        for _, img_batch in tqdm(loader, desc=\"Loading & flattening\"):\n",
    "            flat = img_batch.flatten(1)\n",
    "            flats.append(flat.to(to_device) if to_device else flat)\n",
    "        return torch.cat(flats, dim=0)\n",
    "\n",
    "    # 1) Load & flatten UNLEARN set (small M) → GPU\n",
    "    u_imgs = load_flat(unlearn_indices, to_device=device)  # [M, D]\n",
    "\n",
    "    # 2) If Mahalanobis, precompute global whitening on ALL backups\n",
    "    if score_type.lower() == \"mahalanobis\":\n",
    "        # 2a) flatten all backups on CPU\n",
    "        b_all = load_flat(backup_indices, to_device=None).double()  # [K, D] on CPU\n",
    "        mu = b_all.mean(dim=0, keepdim=True)  # [1, D]\n",
    "        Xc = b_all - mu\n",
    "        cov = (Xc.t() @ Xc) / (b_all.size(0) - 1)  # [D, D]\n",
    "        cov += 1e-5 * torch.eye(cov.size(0))  # regularize\n",
    "        inv_L = torch.linalg.cholesky(torch.inverse(cov))  # [D, D] on CPU\n",
    "\n",
    "        # Move whitening to GPU and whiten u_imgs once\n",
    "        mu_dev = mu.to(device)\n",
    "        inv_L_dev = inv_L.to(device)\n",
    "        u_imgs = (u_imgs - mu_dev) @ inv_L_dev.T  # [M, D] ← now whitened\n",
    "\n",
    "    # 3) Stream through backup chunks on GPU to compute distances\n",
    "    ds = IndexedDataset(dataset, backup_indices)\n",
    "    loader = DataLoader(\n",
    "        ds, batch_size=batch_size, num_workers=4, shuffle=False, pin_memory=True\n",
    "    )\n",
    "\n",
    "    dists_chunks = []\n",
    "    for _, b_img_batch in tqdm(loader, desc=\"Streaming BACKUP\"):\n",
    "        b_flat = b_img_batch.flatten(1).to(device)  # [B, D]\n",
    "\n",
    "        if score_type.lower() == \"l2\":\n",
    "            # Euclidean on raw pixels\n",
    "            d_chunk = torch.cdist(u_imgs, b_flat, p=2)  # [M, B]\n",
    "\n",
    "        else:\n",
    "            # Mahalanobis: whiten this chunk and compute Euclid.\n",
    "            b_w = (b_flat - mu_dev) @ inv_L_dev.T  # [B, D]\n",
    "            d_chunk = torch.cdist(u_imgs, b_w, p=2)  # [M, B]\n",
    "\n",
    "        dists_chunks.append(d_chunk)\n",
    "\n",
    "    # 4) Build full [M, K] distance matrix\n",
    "    dists = torch.cat(dists_chunks, dim=1)  # [M, K]\n",
    "\n",
    "    # 5) Greedy “first-free” matching to get 25 unique twins\n",
    "    mapping, similarity = {}, {}\n",
    "    used = torch.zeros(dists.size(1), dtype=torch.bool, device=device)\n",
    "\n",
    "    for i in range(dists.size(0)):\n",
    "        for cand in torch.argsort(dists[i]):  # lowest‐distance first\n",
    "            if not used[cand]:\n",
    "                ui = unlearn_indices[i]\n",
    "                bi = backup_indices[cand.item()]\n",
    "                mapping[ui] = bi\n",
    "                similarity[ui] = float(dists[i, cand].item())\n",
    "                used[cand] = True\n",
    "                break\n",
    "\n",
    "    return mapping, similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T17:33:46.062320Z",
     "iopub.status.busy": "2025-05-06T17:33:46.062046Z",
     "iopub.status.idle": "2025-05-06T17:33:46.073615Z",
     "shell.execute_reply": "2025-05-06T17:33:46.072960Z",
     "shell.execute_reply.started": "2025-05-06T17:33:46.062300Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def find_unique_shadow_twins_features(\n",
    "    model,\n",
    "    dataset,\n",
    "    backup_indices,\n",
    "    unlearn_indices,\n",
    "    score_type=\"cosine\",  # \"cosine\" or \"mahalanobis\"\n",
    "    batch_size=256,\n",
    "    device=\"cuda:0\",\n",
    "):\n",
    "    # 1) Unwrap & move to single GPU\n",
    "    base_model = model.module if hasattr(model, \"module\") else model\n",
    "    base_model = base_model.to(device).eval()\n",
    "\n",
    "    # 2) Hook avgpool — overwrite output every batch\n",
    "    hook_out = {}\n",
    "    hook = base_model.fc1.register_forward_hook(\n",
    "        lambda module, inp, out: hook_out.update({\"f\": out})\n",
    "    )\n",
    "\n",
    "    # 3) Feature extractor helper\n",
    "    def extract_feats(indices):\n",
    "        ds = IndexedDataset(dataset, indices)\n",
    "        loader = DataLoader(\n",
    "            ds, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True\n",
    "        )\n",
    "        idxs_list, feats_list = [], []\n",
    "        with torch.no_grad():\n",
    "            for idx_batch, img_batch in tqdm(loader, desc=\"Extracting features\"):\n",
    "                imgs = img_batch.to(device, non_blocking=True)\n",
    "                _ = base_model(imgs)\n",
    "                feats = hook_out[\"f\"].flatten(1)  # [B, F]\n",
    "                idxs_list.append(idx_batch.to(device))\n",
    "                feats_list.append(feats)\n",
    "        idxs = torch.cat(idxs_list, dim=0)\n",
    "        feats = torch.cat(feats_list, dim=0)\n",
    "        return idxs, feats\n",
    "\n",
    "    # 4) Extract unlearn & backup features\n",
    "    u_idx, u_feat = extract_feats(unlearn_indices)  # [M], [M, F]\n",
    "    b_idx, b_feat = extract_feats(backup_indices)  # [K], [K, F]\n",
    "    hook.remove()\n",
    "\n",
    "    # 5) Compute similarity/distance matrix\n",
    "    if score_type == \"cosine\":\n",
    "        u_n = F.normalize(u_feat, dim=1)  # [M, F]\n",
    "        b_n = F.normalize(b_feat, dim=1)  # [K, F]\n",
    "        matrix = u_n @ b_n.t()  # higher = more similar\n",
    "        higher_is_better = True\n",
    "\n",
    "    elif score_type == \"mahalanobis\":\n",
    "        # Cast to double for stable covariance\n",
    "        b_d = b_feat.double()\n",
    "        mean_d = b_d.mean(0, keepdim=True)  # [1, F]\n",
    "        Xc = b_d - mean_d\n",
    "        cov = (Xc.t() @ Xc) / (b_d.size(0) - 1)\n",
    "        cov += 1e-5 * torch.eye(cov.size(0), device=device, dtype=torch.double)\n",
    "        inv_L_d = torch.linalg.cholesky(torch.inverse(cov))\n",
    "        inv_L = inv_L_d.to(torch.float32)\n",
    "\n",
    "        # Whiten features\n",
    "        mean_f = mean_d.to(torch.float32)\n",
    "        u_w = (u_feat - mean_f) @ inv_L.t()  # [M, F]\n",
    "        b_w = (b_feat - mean_f) @ inv_L.t()  # [K, F]\n",
    "        matrix = torch.cdist(u_w, b_w, p=2)  # lower = more similar\n",
    "        higher_is_better = False\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"score_type must be 'cosine' or 'mahalanobis'\")\n",
    "\n",
    "    # 6) Greedy “first-free” unique matching\n",
    "    M, K = matrix.shape\n",
    "    used = torch.zeros(K, dtype=torch.bool, device=device)\n",
    "    mapping, similarity = {}, {}\n",
    "    for i in range(M):\n",
    "        row = matrix[i]\n",
    "        order = torch.argsort(-row) if higher_is_better else torch.argsort(row)\n",
    "        for c in order:\n",
    "            idx_c = c.item()\n",
    "            if not used[idx_c]:\n",
    "                ui = u_idx[i].item()\n",
    "                bi = b_idx[idx_c].item()\n",
    "                mapping[ui] = bi\n",
    "                similarity[ui] = float(row[idx_c].item())\n",
    "                used[idx_c] = True\n",
    "                break\n",
    "    torch.set_grad_enabled(True)\n",
    "    return mapping, similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple base training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T17:33:54.023707Z",
     "iopub.status.busy": "2025-05-06T17:33:54.023400Z",
     "iopub.status.idle": "2025-05-06T17:33:54.261121Z",
     "shell.execute_reply": "2025-05-06T17:33:54.260330Z",
     "shell.execute_reply.started": "2025-05-06T17:33:54.023685Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model, model_name, criterion, optimizer, transform = init_model_cnn(\n",
    "    learning_rate=LEARNING_RATE\n",
    ")\n",
    "model_name = \"base\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T17:33:55.701596Z",
     "iopub.status.busy": "2025-05-06T17:33:55.701024Z",
     "iopub.status.idle": "2025-05-06T17:33:55.799838Z",
     "shell.execute_reply": "2025-05-06T17:33:55.799267Z",
     "shell.execute_reply.started": "2025-05-06T17:33:55.701573Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_dataset_full = datasets.MNIST(\n",
    "    root=data_root, train=True, transform=transform, download=True\n",
    ")\n",
    "test_dataset = datasets.MNIST(\n",
    "    root=data_root, train=False, transform=transform, download=True\n",
    ")\n",
    "data_split_path = \"mnist_data_splits.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Representative & Backup Splits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T17:33:58.603636Z",
     "iopub.status.busy": "2025-05-06T17:33:58.603075Z",
     "iopub.status.idle": "2025-05-06T17:33:58.632740Z",
     "shell.execute_reply": "2025-05-06T17:33:58.632123Z",
     "shell.execute_reply.started": "2025-05-06T17:33:58.603612Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "rep_indices, backup_indices = select_representative_and_backup_indices(\n",
    "    train_dataset_full, rep_size=40000\n",
    ")\n",
    "print(\n",
    "    f\"Representative subset size: {len(rep_indices)}; Backup subset size: {len(backup_indices)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create representative subset based on the selected indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T17:34:00.693387Z",
     "iopub.status.busy": "2025-05-06T17:34:00.693021Z",
     "iopub.status.idle": "2025-05-06T17:34:00.697541Z",
     "shell.execute_reply": "2025-05-06T17:34:00.696794Z",
     "shell.execute_reply.started": "2025-05-06T17:34:00.693359Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "rep_subset = Subset(train_dataset_full, rep_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init dataloders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T17:34:02.927304Z",
     "iopub.status.busy": "2025-05-06T17:34:02.927036Z",
     "iopub.status.idle": "2025-05-06T17:34:02.946969Z",
     "shell.execute_reply": "2025-05-06T17:34:02.946314Z",
     "shell.execute_reply.started": "2025-05-06T17:34:02.927286Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_loader, valid_loader, test_loader, classes = init_dataloaders(\n",
    "    datasets=(rep_subset, test_dataset), val_ratio=0.2, batch_size=BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Call train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T17:34:06.183426Z",
     "iopub.status.busy": "2025-05-06T17:34:06.183176Z",
     "iopub.status.idle": "2025-05-06T17:35:29.369231Z",
     "shell.execute_reply": "2025-05-06T17:35:29.368532Z",
     "shell.execute_reply.started": "2025-05-06T17:34:06.183411Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.perf_counter()\n",
    "train_model(\n",
    "    model,\n",
    "    model_name,\n",
    "    train_loader,\n",
    "    valid_loader,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    num_epochs=EPOCHS,\n",
    ")\n",
    "end_time = time.perf_counter()  # End timer\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "print(f\"Execution time: {elapsed_time:.6f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot history losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T17:37:45.279625Z",
     "iopub.status.busy": "2025-05-06T17:37:45.279280Z",
     "iopub.status.idle": "2025-05-06T17:37:45.697495Z",
     "shell.execute_reply": "2025-05-06T17:37:45.696603Z",
     "shell.execute_reply.started": "2025-05-06T17:37:45.279605Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "history_path = f\"{model_name}_history.json\"\n",
    "plot_training_history(history_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Call test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T17:37:48.732398Z",
     "iopub.status.busy": "2025-05-06T17:37:48.732109Z",
     "iopub.status.idle": "2025-05-06T17:37:50.361793Z",
     "shell.execute_reply": "2025-05-06T17:37:50.361170Z",
     "shell.execute_reply.started": "2025-05-06T17:37:48.732377Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model_path = f\"{model_name}_model.pth\"\n",
    "test_model(model, model_name, model_path, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T17:37:51.672042Z",
     "iopub.status.busy": "2025-05-06T17:37:51.671728Z",
     "iopub.status.idle": "2025-05-06T17:37:52.113047Z",
     "shell.execute_reply": "2025-05-06T17:37:52.112150Z",
     "shell.execute_reply.started": "2025-05-06T17:37:51.672022Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "predictions_path = f\"{model_name}_predictions.json\"\n",
    "# classes = ['0 - zero', '1 - one', '2 - two', '3 - three', '4 - four', '5 - five', '6 - six', '7 - seven', '8 - eight', '9 - nine']\n",
    "show_metrics(predictions_path, classes, model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pick samples to unlearn & their shadow twins\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the full dataset with rep_indices to select unlearn samples.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert these original indices to indices relative to the representative subset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T17:37:55.339338Z",
     "iopub.status.busy": "2025-05-06T17:37:55.339057Z",
     "iopub.status.idle": "2025-05-06T17:37:59.858270Z",
     "shell.execute_reply": "2025-05-06T17:37:59.857475Z",
     "shell.execute_reply.started": "2025-05-06T17:37:55.339319Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"Identifying {NUM_UNLEARN_SAMPLES_LOW_LOGIT} samples with the lowest true-class logits from the representative set...\"\n",
    ")\n",
    "unlearn_original_indices = get_lowest_logit_samples(\n",
    "    model=model,\n",
    "    full_dataset=train_dataset_full,\n",
    "    subset_indices=rep_indices,\n",
    "    num_samples=NUM_UNLEARN_SAMPLES_LOW_LOGIT,\n",
    "    device=DEVICE,\n",
    "    batch_size=BATCH_SIZE,\n",
    ")\n",
    "print(\n",
    "    f\"Selected {len(unlearn_original_indices)} unlearn sample original indices (from full dataset) based on lowest logits.\"\n",
    ")\n",
    "rep_indices_map = {orig_idx: i for i, orig_idx in enumerate(rep_indices)}\n",
    "unlearn_relative_indices = []\n",
    "valid_unlearn_original_indices = []\n",
    "\n",
    "for u_orig_idx in unlearn_original_indices:\n",
    "    if u_orig_idx in rep_indices_map:\n",
    "        unlearn_relative_indices.append(rep_indices_map[u_orig_idx])\n",
    "        valid_unlearn_original_indices.append(u_orig_idx)\n",
    "    else:\n",
    "        print(\n",
    "            f\"Warning: Low-logit sample {u_orig_idx} not in rep_indices. This shouldn't happen if get_lowest_logit_samples used rep_indices.\"\n",
    "        )\n",
    "\n",
    "unlearn_original_indices = valid_unlearn_original_indices\n",
    "print(\n",
    "    f\"Unlearn sample indices relative to representative subset ({len(unlearn_relative_indices)}): {unlearn_relative_indices[:10]}...\"\n",
    ")\n",
    "print(f\"Number of valid unlearn samples to process: {len(unlearn_original_indices)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unique Shadow Twin Matching\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T17:38:27.515920Z",
     "iopub.status.busy": "2025-05-06T17:38:27.515601Z",
     "iopub.status.idle": "2025-05-06T17:38:27.521372Z",
     "shell.execute_reply": "2025-05-06T17:38:27.520635Z",
     "shell.execute_reply.started": "2025-05-06T17:38:27.515880Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "import csv\n",
    "\n",
    "\n",
    "def save_shadow_mapping(mapping, similarity, json_path, csv_path):\n",
    "    with open(json_path, \"w\") as f:\n",
    "        json.dump(mapping, f, indent=4)\n",
    "    with open(csv_path, \"w\", newline=\"\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"unlearn_idx\", \"backup_idx\", \"score\"])\n",
    "        for un, back in mapping.items():\n",
    "            writer.writerow([un, back, similarity[un]])\n",
    "    print(f\"Saved JSON → {json_path} and CSV → {csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T17:38:29.376794Z",
     "iopub.status.busy": "2025-05-06T17:38:29.376197Z",
     "iopub.status.idle": "2025-05-06T17:39:01.046260Z",
     "shell.execute_reply": "2025-05-06T17:39:01.045422Z",
     "shell.execute_reply.started": "2025-05-06T17:38:29.376771Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "twin_mappings_list = []\n",
    "\n",
    "if not unlearn_original_indices:\n",
    "    print(\"No unlearn samples identified. Skipping twin finding.\")\n",
    "else:\n",
    "    # RAW L2 (Class-Constrained)\n",
    "    print(\"\\n--- Finding Twins: RAW L2 (Class-Constrained) ---\")\n",
    "    start_time = time.perf_counter()\n",
    "    raw_l2_mapping, raw_l2_similarity = (\n",
    "        find_class_constrained_shadow_twins_batched_per_class(\n",
    "            twin_finder_func=find_unique_shadow_twins_raw,\n",
    "            model=None,\n",
    "            full_dataset=train_dataset_full,\n",
    "            all_backup_indices=backup_indices,\n",
    "            unlearn_original_indices=unlearn_original_indices,\n",
    "            score_type=\"l2\",\n",
    "            batch_size=512,\n",
    "            device=DEVICE,\n",
    "        )\n",
    "    )\n",
    "    end_time = time.perf_counter()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Execution time for RAW L2 (Class-Constrained): {elapsed_time:.6f} seconds\")\n",
    "    save_shadow_mapping(\n",
    "        raw_l2_mapping,\n",
    "        raw_l2_similarity,\n",
    "        \"raw_l2_class_constrained_mapping.json\",\n",
    "        \"raw_l2_class_constrained_similarity.csv\",\n",
    "    )\n",
    "    if raw_l2_mapping:\n",
    "        twin_mappings_list.append(\n",
    "            (\"raw_l2_class_constrained\", raw_l2_mapping, raw_l2_similarity)\n",
    "        )\n",
    "\n",
    "    # RAW MAHALANOBIS (Class-Constrained)\n",
    "    print(\"\\n--- Finding Twins: RAW MAHALANOBIS (Class-Constrained) ---\")\n",
    "    start_time = time.perf_counter()\n",
    "    raw_mahalanobis_mapping, raw_mahalanobis_similarity = (\n",
    "        find_class_constrained_shadow_twins_batched_per_class(\n",
    "            twin_finder_func=find_unique_shadow_twins_raw,\n",
    "            model=None,\n",
    "            full_dataset=train_dataset_full,\n",
    "            all_backup_indices=backup_indices,\n",
    "            unlearn_original_indices=unlearn_original_indices,\n",
    "            score_type=\"mahalanobis\",\n",
    "            batch_size=512,\n",
    "            device=DEVICE,\n",
    "        )\n",
    "    )\n",
    "    end_time = time.perf_counter()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(\n",
    "        f\"Execution time for RAW MAHALANOBIS (Class-Constrained): {elapsed_time:.6f} seconds\"\n",
    "    )\n",
    "    save_shadow_mapping(\n",
    "        raw_mahalanobis_mapping,\n",
    "        raw_mahalanobis_similarity,\n",
    "        \"raw_mahalanobis_class_constrained_mapping.json\",\n",
    "        \"raw_mahalanobis_class_constrained_similarity.csv\",\n",
    "    )\n",
    "    if raw_mahalanobis_mapping:\n",
    "        twin_mappings_list.append(\n",
    "            (\n",
    "                \"raw_mahalanobis_class_constrained\",\n",
    "                raw_mahalanobis_mapping,\n",
    "                raw_mahalanobis_similarity,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # FEATURE COSINE (Class-Constrained)\n",
    "    print(\"\\n--- Finding Twins: FEATURE COSINE (Class-Constrained) ---\")\n",
    "    start_time = time.perf_counter()\n",
    "    features_cosine_mapping, features_cosine_similarity = (\n",
    "        find_class_constrained_shadow_twins_batched_per_class(\n",
    "            twin_finder_func=find_unique_shadow_twins_features,\n",
    "            model=model,\n",
    "            full_dataset=train_dataset_full,\n",
    "            all_backup_indices=backup_indices,\n",
    "            unlearn_original_indices=unlearn_original_indices,\n",
    "            score_type=\"cosine\",\n",
    "            batch_size=512,\n",
    "            device=DEVICE,\n",
    "        )\n",
    "    )\n",
    "    end_time = time.perf_counter()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(\n",
    "        f\"Execution time for FEATURE COSINE (Class-Constrained): {elapsed_time:.6f} seconds\"\n",
    "    )\n",
    "    save_shadow_mapping(\n",
    "        features_cosine_mapping,\n",
    "        features_cosine_similarity,\n",
    "        \"feat_cosine_class_constrained_mapping.json\",\n",
    "        \"feat_cosine_class_constrained_similarity.csv\",\n",
    "    )\n",
    "    if features_cosine_mapping:\n",
    "        twin_mappings_list.append(\n",
    "            (\n",
    "                \"features_cosine_class_constrained\",\n",
    "                features_cosine_mapping,\n",
    "                features_cosine_similarity,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # FEATURE MAHALANOBIS (Class-Constrained)\n",
    "    print(\"\\n--- Finding Twins: FEATURE MAHALANOBIS (Class-Constrained) ---\")\n",
    "    start_time = time.perf_counter()\n",
    "    features_mahalanobis_mapping, features_mahalanobis_similarity = (\n",
    "        find_class_constrained_shadow_twins_batched_per_class(\n",
    "            twin_finder_func=find_unique_shadow_twins_features,\n",
    "            model=model,\n",
    "            full_dataset=train_dataset_full,\n",
    "            all_backup_indices=backup_indices,\n",
    "            unlearn_original_indices=unlearn_original_indices,\n",
    "            score_type=\"mahalanobis\",\n",
    "            batch_size=512,\n",
    "            device=DEVICE,\n",
    "        )\n",
    "    )\n",
    "    end_time = time.perf_counter()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(\n",
    "        f\"Execution time for FEATURE MAHALANOBIS (Class-Constrained): {elapsed_time:.6f} seconds\"\n",
    "    )\n",
    "    save_shadow_mapping(\n",
    "        features_mahalanobis_mapping,\n",
    "        features_mahalanobis_similarity,\n",
    "        \"feat_mahalanobis_class_constrained_mapping.json\",\n",
    "        \"feat_mahalanobis_class_constrained_similarity.csv\",\n",
    "    )\n",
    "    if features_mahalanobis_mapping:\n",
    "        twin_mappings_list.append(\n",
    "            (\n",
    "                \"features_mahalanobis_class_constrained\",\n",
    "                features_mahalanobis_mapping,\n",
    "                features_mahalanobis_similarity,\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T18:01:15.210108Z",
     "iopub.status.busy": "2025-05-06T18:01:15.209322Z",
     "iopub.status.idle": "2025-05-06T18:01:30.428451Z",
     "shell.execute_reply": "2025-05-06T18:01:30.427686Z",
     "shell.execute_reply.started": "2025-05-06T18:01:15.210084Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "for method_name, mapping, _ in twin_mappings_list:\n",
    "    fig, axes = plt.subplots(5, 10, figsize=(20, 10))\n",
    "    axes = axes.reshape(5, 10)\n",
    "    fig.suptitle(method_name, fontsize=16)\n",
    "\n",
    "    for i, (a_idx, b_idx) in enumerate(mapping.items()):\n",
    "        row = i // 5\n",
    "        col_offset = (i % 5) * 2\n",
    "\n",
    "        # Plot A\n",
    "        ax = axes[row, col_offset]\n",
    "        ax.imshow(train_dataset_full.data[a_idx])\n",
    "        ax.set_title(f\"Orgiinal: {a_idx}\", fontsize=8)\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "        # Plot B\n",
    "        ax = axes[row, col_offset + 1]\n",
    "        ax.imshow(train_dataset_full.data[b_idx])\n",
    "        ax.set_title(f\"Replacement: {b_idx}\", fontsize=8)\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    plt.savefig(f\"{method_name}_twins.jpg\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T17:39:23.139184Z",
     "iopub.status.busy": "2025-05-06T17:39:23.138620Z",
     "iopub.status.idle": "2025-05-06T17:39:23.449174Z",
     "shell.execute_reply": "2025-05-06T17:39:23.448448Z",
     "shell.execute_reply.started": "2025-05-06T17:39:23.139161Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "import gc\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gold naive retraining\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init new model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T17:39:45.502085Z",
     "iopub.status.busy": "2025-05-06T17:39:45.501717Z",
     "iopub.status.idle": "2025-05-06T17:39:45.512136Z",
     "shell.execute_reply": "2025-05-06T17:39:45.511358Z",
     "shell.execute_reply.started": "2025-05-06T17:39:45.502061Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "gold_model, gold_model_name, criterion, optimizer, transform = init_model_cnn(\n",
    "    learning_rate=LEARNING_RATE\n",
    ")\n",
    "gold_model_name = \"gold\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get clean gold subset w/o samples to delete\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T17:39:47.548665Z",
     "iopub.status.busy": "2025-05-06T17:39:47.548004Z",
     "iopub.status.idle": "2025-05-06T17:39:47.557848Z",
     "shell.execute_reply": "2025-05-06T17:39:47.557069Z",
     "shell.execute_reply.started": "2025-05-06T17:39:47.548640Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "gold_rep_indices = sorted(list(set(rep_indices) - set(unlearn_original_indices)))\n",
    "print(f\"Clean representative indices count: {len(gold_rep_indices)}\")\n",
    "gold_subset = Subset(train_dataset_full, gold_rep_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init dataloaders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T17:39:49.483668Z",
     "iopub.status.busy": "2025-05-06T17:39:49.483375Z",
     "iopub.status.idle": "2025-05-06T17:39:49.492645Z",
     "shell.execute_reply": "2025-05-06T17:39:49.491788Z",
     "shell.execute_reply.started": "2025-05-06T17:39:49.483647Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "gold_train_loader, gold_valid_loader, gold_test_loader, classes = init_dataloaders(\n",
    "    datasets=(gold_subset, test_dataset), val_ratio=0.2, batch_size=BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Call train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T17:39:51.056374Z",
     "iopub.status.busy": "2025-05-06T17:39:51.055755Z",
     "iopub.status.idle": "2025-05-06T17:41:12.617473Z",
     "shell.execute_reply": "2025-05-06T17:41:12.616747Z",
     "shell.execute_reply.started": "2025-05-06T17:39:51.056349Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.perf_counter()\n",
    "train_model(\n",
    "    gold_model,\n",
    "    gold_model_name,\n",
    "    gold_train_loader,\n",
    "    gold_valid_loader,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    num_epochs=EPOCHS,\n",
    ")\n",
    "end_time = time.perf_counter()  # End timer\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "print(f\"Execution time: {elapsed_time:.6f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot history losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T17:41:36.517703Z",
     "iopub.status.busy": "2025-05-06T17:41:36.517389Z",
     "iopub.status.idle": "2025-05-06T17:41:36.863875Z",
     "shell.execute_reply": "2025-05-06T17:41:36.863194Z",
     "shell.execute_reply.started": "2025-05-06T17:41:36.517684Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "history_path = f\"{gold_model_name}_history.json\"\n",
    "plot_training_history(history_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Call test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T17:41:39.299310Z",
     "iopub.status.busy": "2025-05-06T17:41:39.298781Z",
     "iopub.status.idle": "2025-05-06T17:41:40.921540Z",
     "shell.execute_reply": "2025-05-06T17:41:40.920620Z",
     "shell.execute_reply.started": "2025-05-06T17:41:39.299284Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "gold_model_path = f\"{gold_model_name}_model.pth\"\n",
    "test_model(gold_model, gold_model_name, gold_model_path, gold_test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T17:41:43.173985Z",
     "iopub.status.busy": "2025-05-06T17:41:43.173357Z",
     "iopub.status.idle": "2025-05-06T17:41:43.576874Z",
     "shell.execute_reply": "2025-05-06T17:41:43.576216Z",
     "shell.execute_reply.started": "2025-05-06T17:41:43.173944Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "predictions_path = f\"{gold_model_name}_predictions.json\"\n",
    "# classes = ['0 - zero', '1 - one', '2 - two', '3 - three', '4 - four', '5 - five', '6 - six', '7 - seven', '8 - eight', '9 - nine']\n",
    "show_metrics(predictions_path, classes, model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unlearning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init new model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T17:42:39.477088Z",
     "iopub.status.busy": "2025-05-06T17:42:39.476302Z",
     "iopub.status.idle": "2025-05-06T17:42:39.492848Z",
     "shell.execute_reply": "2025-05-06T17:42:39.492256Z",
     "shell.execute_reply.started": "2025-05-06T17:42:39.477060Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model_file = project_root / f\"experiments/mnist/naive/{model_name}_model.pth\"\n",
    "original_model, original_model_name, criterion, _optimizer, transform = load_model_cnn(\n",
    "    model_pth_path=model_file\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unlearning via fisher\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T17:42:41.347559Z",
     "iopub.status.busy": "2025-05-06T17:42:41.346938Z",
     "iopub.status.idle": "2025-05-06T17:42:41.367534Z",
     "shell.execute_reply": "2025-05-06T17:42:41.366947Z",
     "shell.execute_reply.started": "2025-05-06T17:42:41.347539Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from utils.utils import DEVICE\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "\n",
    "def compute_gradient_on_subset(model, criterion, dataset_subset, batch_size):\n",
    "    \"\"\"\n",
    "    Compute the average gradient Δ_rem = ∇L(θ, D') over the given dataset_subset.\n",
    "    \"\"\"\n",
    "    dataloader = DataLoader(dataset_subset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    grad_dict = {}\n",
    "    total_samples = 0\n",
    "\n",
    "    model.train()\n",
    "    for inputs, targets in tqdm(dataloader, desc=\"Computing gradients\"):\n",
    "        inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
    "        model.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        current_batch = inputs.size(0)\n",
    "        total_samples += current_batch\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.requires_grad and param.grad is not None:\n",
    "                if name not in grad_dict:\n",
    "                    grad_dict[name] = param.grad.detach().clone() * current_batch\n",
    "                else:\n",
    "                    grad_dict[name] += param.grad.detach() * current_batch\n",
    "\n",
    "    # Average gradients over the entire subset\n",
    "    for name in grad_dict:\n",
    "        grad_dict[name] /= total_samples\n",
    "\n",
    "    return grad_dict\n",
    "\n",
    "\n",
    "def compute_fisher_on_subset(model, criterion, dataset_subset, batch_size):\n",
    "    \"\"\"\n",
    "    Compute a diagonal approximation of the Fisher Information Matrix F over the given dataset_subset.\n",
    "    It averages the squared gradients.\n",
    "    \"\"\"\n",
    "    dataloader = DataLoader(dataset_subset, batch_size=batch_size, shuffle=False)\n",
    "    fisher_diag = {}\n",
    "    total_samples = 0\n",
    "\n",
    "    model.eval()\n",
    "    for inputs, targets in tqdm(dataloader, desc=\"Computing Fisher\"):\n",
    "        inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
    "        model.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        current_batch = inputs.size(0)\n",
    "        total_samples += current_batch\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.requires_grad and param.grad is not None:\n",
    "                if name not in fisher_diag:\n",
    "                    fisher_diag[name] = (param.grad.detach() ** 2) * current_batch\n",
    "                else:\n",
    "                    fisher_diag[name] += (param.grad.detach() ** 2) * current_batch\n",
    "\n",
    "    for name in fisher_diag:\n",
    "        fisher_diag[name] /= total_samples\n",
    "\n",
    "    return fisher_diag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T17:42:45.010746Z",
     "iopub.status.busy": "2025-05-06T17:42:45.010168Z",
     "iopub.status.idle": "2025-05-06T17:42:45.018303Z",
     "shell.execute_reply": "2025-05-06T17:42:45.017531Z",
     "shell.execute_reply.started": "2025-05-06T17:42:45.010725Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def iterative_fisher_unlearn_simplified_golatkar(\n",
    "    model,\n",
    "    criterion,\n",
    "    full_dataset,\n",
    "    removal_indices,\n",
    "    sigma_lambda_term,\n",
    "    deletion_batch_size,\n",
    "    compute_batch_size,\n",
    "    eps=1e-5,\n",
    "):\n",
    "\n",
    "    full_size = len(full_dataset)\n",
    "    current_indices = set(range(full_size))\n",
    "\n",
    "    removal_list = list(removal_indices)\n",
    "    num_batches = math.ceil(len(removal_list) / deletion_batch_size)\n",
    "    partitioned_removals = [\n",
    "        removal_list[i * deletion_batch_size : (i + 1) * deletion_batch_size]\n",
    "        for i in range(num_batches)\n",
    "    ]\n",
    "    print(f\"Applying simplified Golatkar Fisher (Iterative Noise Injection)...\")\n",
    "    print(\n",
    "        f\"Total deletion samples: {len(removal_list)}; partitioned into {num_batches} mini-batches (each up to {deletion_batch_size} samples).\"\n",
    "    )\n",
    "    print(f\"Noise scale term (λσ_h): {sigma_lambda_term}\")\n",
    "\n",
    "    model.to(DEVICE)\n",
    "    for i, batch in enumerate(\n",
    "        tqdm(partitioned_removals, desc=\"Iterative Fisher Noise Injection\")\n",
    "    ):\n",
    "        current_indices -= set(batch)\n",
    "        updated_indices = sorted(list(current_indices))\n",
    "        dataset_remaining = Subset(full_dataset, updated_indices)\n",
    "        print(\n",
    "            f\"\\nIteration {i+1}/{num_batches}: Remaining dataset size = {len(dataset_remaining)}\"\n",
    "        )\n",
    "\n",
    "        model.eval()\n",
    "        fisher_diag = compute_fisher_on_subset(\n",
    "            model, criterion, dataset_remaining, compute_batch_size\n",
    "        )\n",
    "\n",
    "        model.train()\n",
    "        with torch.no_grad():\n",
    "            total_noise_norm_iter = 0.0\n",
    "            for name, param in model.named_parameters():\n",
    "                if param.requires_grad and name in fisher_diag:\n",
    "\n",
    "                    # Calculate F^(-1/4) with damping\n",
    "                    inv_fisher_quarter = (fisher_diag[name].to(DEVICE) + eps).pow(-0.25)\n",
    "\n",
    "                    # Generate Gaussian noise n ~ N(0, I)\n",
    "                    noise = torch.randn_like(param.data)\n",
    "\n",
    "                    # Calculate the noise term: (λσ_h) * F^(-1/4) *\n",
    "                    scaled_noise = sigma_lambda_term * inv_fisher_quarter * noise\n",
    "                    noise_norm = scaled_noise.norm(2).item()\n",
    "                    total_noise_norm_iter += noise_norm**2\n",
    "\n",
    "                    param.data.add_(scaled_noise)\n",
    "\n",
    "        print(\n",
    "            f\"Iteration {i+1}: Total added noise norm (sqrt sum sq) = {total_noise_norm_iter**0.5:.4e}\"\n",
    "        )\n",
    "        print(f\"Iteration {i+1}/{num_batches} noise injection completed.\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-05-01T12:21:51.916590Z",
     "iopub.status.busy": "2025-05-01T12:21:51.915868Z",
     "iopub.status.idle": "2025-05-01T12:21:58.765671Z",
     "shell.execute_reply": "2025-05-01T12:21:58.765093Z",
     "shell.execute_reply.started": "2025-05-01T12:21:51.916553Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "COMPUTE_BATCH_SIZE = 512\n",
    "MINI_BATCH_SIZE = 25\n",
    "\n",
    "SIGMA = 35e-4\n",
    "\n",
    "EPS = 1e-6\n",
    "MAX_NORM = 5\n",
    "\n",
    "import copy\n",
    "\n",
    "model_to_unlearn_name = \"fisher\"\n",
    "model_to_unlearn = copy.deepcopy(original_model)\n",
    "\n",
    "# import torch.nn as nn\n",
    "# model_to_unlearn = nn.DataParallel(model_to_unlearn, device_ids=[0, 1])\n",
    "\n",
    "import time\n",
    "\n",
    "start_time = time.perf_counter()\n",
    "\n",
    "unlearned_model = iterative_fisher_unlearn_simplified_golatkar(\n",
    "    model_to_unlearn,\n",
    "    criterion,\n",
    "    rep_subset,\n",
    "    unlearn_relative_indices,\n",
    "    SIGMA,\n",
    "    deletion_batch_size=MINI_BATCH_SIZE,\n",
    "    compute_batch_size=COMPUTE_BATCH_SIZE,\n",
    "    eps=EPS,\n",
    "    # max_norm=MAX_NORM\n",
    ")\n",
    "\n",
    "end_time = time.perf_counter()  # End timer\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "print(f\"Execution time: {elapsed_time:.6f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unlearning via Influence x Lissa\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T17:42:50.634542Z",
     "iopub.status.busy": "2025-05-06T17:42:50.633971Z",
     "iopub.status.idle": "2025-05-06T17:42:50.659203Z",
     "shell.execute_reply": "2025-05-06T17:42:50.658578Z",
     "shell.execute_reply.started": "2025-05-06T17:42:50.634519Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from tqdm import tqdm, trange\n",
    "from torch.amp import GradScaler, autocast\n",
    "import math\n",
    "\n",
    "# scaler = GradScaler()\n",
    "\n",
    "\n",
    "def compute_gradient_on_subset(model, criterion, dataset_subset, batch_size):\n",
    "    \"\"\"\n",
    "    Compute the average gradient Δ = (1/|D_u|) Σ_{(x,y) in D_u} ∇_θ L(θ, (x,y))\n",
    "    over the dataset_subset.\n",
    "    \"\"\"\n",
    "    dataloader = DataLoader(dataset_subset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    grad_dict = {\n",
    "        name: torch.zeros_like(param, device=DEVICE)\n",
    "        for name, param in model.named_parameters()\n",
    "        if param.requires_grad\n",
    "    }\n",
    "\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.enable_grad():\n",
    "        model.eval()\n",
    "        for inputs, targets in tqdm(dataloader, desc=\"Computing gradients\"):\n",
    "            inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
    "            model.zero_grad()\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            current_batch = inputs.size(0)\n",
    "            total_samples += current_batch\n",
    "\n",
    "            for name, param in model.named_parameters():\n",
    "                if param.requires_grad and param.grad is not None:\n",
    "                    grad_dict[name] += param.grad.detach().clone() * current_batch\n",
    "\n",
    "        # Average over total samples\n",
    "        for name in grad_dict:\n",
    "            grad_dict[name] /= total_samples\n",
    "\n",
    "        # Flatten all gradients into one vector\n",
    "        grad_vector = torch.cat(\n",
    "            [grad_dict[name].view(-1) for name in sorted(grad_dict.keys())]\n",
    "        )\n",
    "    return grad_vector\n",
    "\n",
    "\n",
    "def lissa_inverse_hvp(\n",
    "    model, criterion, data_loader, v, damping=1e-6, scale=100, recursion_depth=20\n",
    "):\n",
    "    convergence_threshold = 1e-4\n",
    "    ihvp_estimate = v.clone().to(DEVICE)\n",
    "    ihvp_prev = torch.zeros_like(ihvp_estimate)\n",
    "    data_iter = iter(data_loader)\n",
    "    model.eval()\n",
    "\n",
    "    # Lists to store metrics\n",
    "    norms_history = []\n",
    "    delta_norms_history = []\n",
    "    relative_change_history = []\n",
    "\n",
    "    for step in trange(recursion_depth, desc=\"LiSSA iterations\", leave=False):\n",
    "        if step > 0:\n",
    "            ihvp_prev = ihvp_estimate.clone()\n",
    "        try:\n",
    "            inputs, targets = next(data_iter)\n",
    "        except StopIteration:\n",
    "            data_iter = iter(data_loader)\n",
    "            inputs, targets = next(data_iter)\n",
    "\n",
    "        inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
    "        model.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        grads = torch.autograd.grad(loss, model.parameters(), create_graph=True)\n",
    "        # Flatten gradients\n",
    "        grad_vector = torch.cat(\n",
    "            [\n",
    "                (\n",
    "                    g.contiguous().view(-1)\n",
    "                    if g is not None\n",
    "                    else torch.zeros(p.numel(), device=DEVICE)\n",
    "                )\n",
    "                for p, g in zip(model.parameters(), grads)\n",
    "            ]\n",
    "        )\n",
    "        if torch.isnan(grad_vector).any() or torch.isinf(grad_vector).any():\n",
    "            return torch.zeros_like(v)\n",
    "\n",
    "        hv = torch.autograd.grad(\n",
    "            torch.dot(grad_vector, ihvp_estimate.detach()),\n",
    "            model.parameters(),\n",
    "            retain_graph=False,\n",
    "        )\n",
    "        # Flatten HVP\n",
    "        hv_vector = torch.cat(\n",
    "            [\n",
    "                (\n",
    "                    h.contiguous().view(-1)\n",
    "                    if h is not None\n",
    "                    else torch.zeros(p.numel(), device=DEVICE)\n",
    "                )\n",
    "                for p, h in zip(model.parameters(), hv)\n",
    "            ]\n",
    "        )\n",
    "        if torch.isnan(hv_vector).any() or torch.isinf(hv_vector).any():\n",
    "            return torch.zeros_like(v)\n",
    "\n",
    "        # --- Update the estimate ---\n",
    "        ihvp_estimate = v + (1 - damping) * ihvp_estimate - hv_vector / scale\n",
    "        if torch.isnan(ihvp_estimate).any() or torch.isinf(ihvp_estimate).any():\n",
    "            return torch.zeros_like(v)\n",
    "\n",
    "        # --- Calculate and Store Metrics ---\n",
    "        current_norm = torch.norm(ihvp_estimate).item()\n",
    "        delta_norm = torch.norm(ihvp_estimate - ihvp_prev).item() if step > 0 else 0.0\n",
    "        prev_norm = torch.norm(ihvp_prev).item() if step > 0 else 0.0\n",
    "        relative_change = (\n",
    "            delta_norm / (prev_norm + 1e-9) if prev_norm > 1e-9 else delta_norm\n",
    "        )\n",
    "\n",
    "        norms_history.append(current_norm)\n",
    "        if step > 0:\n",
    "            delta_norms_history.append(delta_norm)\n",
    "            relative_change_history.append(relative_change)\n",
    "\n",
    "    # --- Plotting Section ---\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.plot(range(recursion_depth), norms_history)\n",
    "    plt.title(\"Estimate Norm vs. Iteration\")\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Norm\")\n",
    "    plt.yscale(\"log\")\n",
    "\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.plot(range(1, recursion_depth), delta_norms_history)\n",
    "    plt.title(\"Absolute Change Norm vs. Iteration\")\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Norm of Change\")\n",
    "    plt.yscale(\"log\")\n",
    "\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.plot(range(1, recursion_depth), relative_change_history)\n",
    "    plt.title(\"Relative Change vs. Iteration\")\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Relative Change\")\n",
    "    plt.yscale(\"log\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    final_norm = norms_history[-1] if norms_history else 0.0\n",
    "    print(f\"Final Estimate Norm: {final_norm:.4e}\")\n",
    "    if math.isnan(final_norm) or math.isinf(final_norm):\n",
    "        print(f\"🚨 ERROR: Final LiSSA estimate norm is NaN/Inf!\")\n",
    "        return torch.zeros_like(v)\n",
    "    else:\n",
    "        print(\"✅ [Debug] LiSSA estimation stable.\")\n",
    "        return ihvp_estimate / scale\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Main Function: Iterative Influence Unlearning\n",
    "# -----------------------------\n",
    "def iterative_influence_unlearn(\n",
    "    model,\n",
    "    criterion,\n",
    "    full_dataset,\n",
    "    removal_indices,\n",
    "    deletion_batch_size,\n",
    "    compute_batch_size,\n",
    "    eps,\n",
    "    max_norm,\n",
    "    scale=100,\n",
    "    lissa_depth=50,\n",
    "):\n",
    "    \"\"\"\n",
    "    Implements iterative Influence Unlearning:\n",
    "      For each mini-batch of deletion samples, compute the average gradient\n",
    "      Δ_u, solve v ≈ H⁻¹ Δ_u using Conjugate Gradient on the remaining data,\n",
    "      and update the model as: θ ← θ + v.\n",
    "\n",
    "    Mathematical Equations:\n",
    "      Δ_u = (1/|D_u^i|) Σ_{(x,y) in D_u^i} ∇_θ L(θ, (x,y))\n",
    "      v ≈ H⁻¹ Δ_u,  where H = ∇²_θ L(θ, D \\ D_u^i)\n",
    "      θ ← θ + v\n",
    "    \"\"\"\n",
    "    full_size = len(full_dataset)\n",
    "    current_indices = set(range(full_size))\n",
    "\n",
    "    # Partition removal_indices into mini-batches\n",
    "    removal_list = list(removal_indices)\n",
    "    num_batches = math.ceil(len(removal_list) / deletion_batch_size)\n",
    "    partitioned_removals = [\n",
    "        removal_list[i * deletion_batch_size : (i + 1) * deletion_batch_size]\n",
    "        for i in range(num_batches)\n",
    "    ]\n",
    "    print(\n",
    "        f\"Total deletion samples: {len(removal_list)}; partitioned into {num_batches} mini-batches (each up to {deletion_batch_size} samples).\"\n",
    "    )\n",
    "\n",
    "    for i, batch in enumerate(tqdm(partitioned_removals, desc=\"Influence Unlearning\")):\n",
    "        # Update remaining indices: D_current ← D \\ D_u^i\n",
    "        current_indices -= set(batch)\n",
    "        updated_indices = sorted(list(current_indices))\n",
    "        dataset_remaining = Subset(full_dataset, updated_indices)\n",
    "        print(\n",
    "            f\"Iteration {i+1}/{num_batches}: Remaining dataset size = {len(dataset_remaining)}\"\n",
    "        )\n",
    "\n",
    "        # Compute average gradient Δ_u for the deletion mini-batch\n",
    "        deleted_subset = Subset(full_dataset, batch)\n",
    "        delta = compute_gradient_on_subset(\n",
    "            model, criterion, deleted_subset, compute_batch_size\n",
    "        )\n",
    "\n",
    "        if torch.isnan(delta).any() or torch.isinf(delta).any():\n",
    "            print(\"🚨 [Debug] Gradient delta contains NaN/Inf!\")\n",
    "        else:\n",
    "            print(\"✅ [Debug] Gradient delta stable.\")\n",
    "\n",
    "        # Create a DataLoader for remaining data to approximate Hessian\n",
    "        remaining_loader = DataLoader(\n",
    "            dataset_remaining, batch_size=compute_batch_size, shuffle=True\n",
    "        )\n",
    "\n",
    "        influence_update = lissa_inverse_hvp(\n",
    "            model,\n",
    "            criterion,\n",
    "            remaining_loader,\n",
    "            delta,\n",
    "            damping=eps,\n",
    "            scale=scale,\n",
    "            recursion_depth=lissa_depth,\n",
    "        )\n",
    "        # Optionally clip the update to avoid overly large changes\n",
    "        if max_norm is not None:\n",
    "            update_norm = influence_update.norm(2).item()\n",
    "            if (\n",
    "                update_norm > max_norm\n",
    "                or math.isnan(update_norm)\n",
    "                or math.isinf(update_norm)\n",
    "            ):\n",
    "                print(\n",
    "                    f\"WARNING: Clipping influence update from {update_norm:.2f} to {max_norm}\"\n",
    "                )\n",
    "                influence_update = influence_update * (max_norm / update_norm)\n",
    "            print(\n",
    "                f\"Iteration {i+1}: Influence update norm = {influence_update.norm(2).item():.4f}\"\n",
    "            )\n",
    "\n",
    "        # Update model parameters: θ ← θ + v\n",
    "        pointer = 0\n",
    "        with torch.no_grad():\n",
    "            for name, param in model.named_parameters():\n",
    "                if param.requires_grad:\n",
    "                    numel = param.numel()\n",
    "                    update_segment = influence_update[\n",
    "                        pointer : pointer + numel\n",
    "                    ].view_as(param)\n",
    "                    param.data = param.data + update_segment\n",
    "                    pointer += numel\n",
    "        print(f\"Iteration {i+1}/{num_batches} update completed.\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-05-01T13:08:39.059555Z",
     "iopub.status.busy": "2025-05-01T13:08:39.058971Z",
     "iopub.status.idle": "2025-05-01T13:08:45.790409Z",
     "shell.execute_reply": "2025-05-01T13:08:45.789782Z",
     "shell.execute_reply.started": "2025-05-01T13:08:39.059533Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "COMPUTE_BATCH_SIZE = 4\n",
    "MINI_BATCH_SIZE = 25\n",
    "\n",
    "EPS = 1e-4\n",
    "MAX_NORM = 5  # 1, 5, 10\n",
    "LISSA_DEPTH = 1000\n",
    "SCALE = 1000\n",
    "\n",
    "import copy\n",
    "\n",
    "model_to_unlearn_name = \"influence\"\n",
    "model_to_unlearn = copy.deepcopy(original_model)\n",
    "\n",
    "# import torch.nn as nn\n",
    "# model_to_unlearn = nn.DataParallel(model_to_unlearn, device_ids=[0, 1])\n",
    "model_to_unlearn_name = \"influence\"\n",
    "model_to_unlearn = copy.deepcopy(original_model)\n",
    "\n",
    "# import torch.nn as nn\n",
    "# model_to_unlearn = nn.DataParallel(model_to_unlearn, device_ids=[0, 1])\n",
    "\n",
    "import time\n",
    "\n",
    "start_time = time.perf_counter()\n",
    "\n",
    "\n",
    "unlearned_model = iterative_influence_unlearn(\n",
    "    model_to_unlearn,\n",
    "    criterion,\n",
    "    rep_subset,\n",
    "    unlearn_relative_indices,\n",
    "    MINI_BATCH_SIZE,\n",
    "    COMPUTE_BATCH_SIZE,\n",
    "    EPS,\n",
    "    MAX_NORM,\n",
    "    SCALE,\n",
    "    LISSA_DEPTH,\n",
    ")\n",
    "\n",
    "end_time = time.perf_counter()  # End timer\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "print(f\"Execution time: {elapsed_time:.6f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-05-01T12:21:58.815462Z",
     "iopub.status.busy": "2025-05-01T12:21:58.815259Z",
     "iopub.status.idle": "2025-05-01T12:21:58.845552Z",
     "shell.execute_reply": "2025-05-01T12:21:58.844976Z",
     "shell.execute_reply.started": "2025-05-01T12:21:58.815447Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "save_model(unlearned_model, f\"{model_to_unlearn_name}_model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test unlearned model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-05-01T12:21:58.846440Z",
     "iopub.status.busy": "2025-05-01T12:21:58.846266Z",
     "iopub.status.idle": "2025-05-01T12:21:58.869718Z",
     "shell.execute_reply": "2025-05-01T12:21:58.868970Z",
     "shell.execute_reply.started": "2025-05-01T12:21:58.846428Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model, *_ = init_model_cnn()\n",
    "# model = nn.DataParallel(model, device_ids=[0, 1])\n",
    "model_path = f\"{model_to_unlearn_name}_model.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-05-01T12:21:58.870763Z",
     "iopub.status.busy": "2025-05-01T12:21:58.870514Z",
     "iopub.status.idle": "2025-05-01T12:21:58.874515Z",
     "shell.execute_reply": "2025-05-01T12:21:58.873809Z",
     "shell.execute_reply.started": "2025-05-01T12:21:58.870740Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "deleted_subset = Subset(train_dataset_full, unlearn_original_indices)\n",
    "deleted_loader = DataLoader(deleted_subset, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-05-01T12:21:58.875299Z",
     "iopub.status.busy": "2025-05-01T12:21:58.875142Z",
     "iopub.status.idle": "2025-05-01T12:22:00.459334Z",
     "shell.execute_reply": "2025-05-01T12:22:00.458648Z",
     "shell.execute_reply.started": "2025-05-01T12:21:58.875286Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test_model(model, model_to_unlearn_name, model_path, gold_test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T17:43:04.438381Z",
     "iopub.status.busy": "2025-05-06T17:43:04.437507Z",
     "iopub.status.idle": "2025-05-06T17:43:04.447588Z",
     "shell.execute_reply": "2025-05-06T17:43:04.446719Z",
     "shell.execute_reply.started": "2025-05-06T17:43:04.438346Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sb\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    confusion_matrix,\n",
    ")\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "def show_metrics(predictions_path, classes, model_name):\n",
    "\n",
    "    with open(predictions_path, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    predictions = np.array(data[\"predictions\"]).flatten().tolist()\n",
    "    true_labels = np.array(data[\"true_labels\"]).flatten().tolist()\n",
    "\n",
    "    # Metrics calculation\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    precision = precision_score(\n",
    "        true_labels, predictions, average=\"weighted\", zero_division=0\n",
    "    )\n",
    "    recall = recall_score(true_labels, predictions, average=\"weighted\", zero_division=0)\n",
    "    f1 = f1_score(true_labels, predictions, average=\"weighted\", zero_division=0)\n",
    "    cm = confusion_matrix(true_labels, predictions)\n",
    "\n",
    "    # Display metrics\n",
    "    print(f\"Metrics for {model_name}:\")\n",
    "    print(f\"  - Test Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"  - Precision: {precision:.4f}\")\n",
    "    print(f\"  - Recall: {recall:.4f}\")\n",
    "    print(f\"  - F1 Score: {f1:.4f}\")\n",
    "\n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sb.heatmap(\n",
    "        cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=classes, yticklabels=classes\n",
    "    )\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.title(f\"Confusion Matrix for {model_name}\")\n",
    "    plt.savefig(f\"{model_name}.jpg\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T17:43:07.191120Z",
     "iopub.status.busy": "2025-05-06T17:43:07.190810Z",
     "iopub.status.idle": "2025-05-06T17:43:07.271689Z",
     "shell.execute_reply": "2025-05-06T17:43:07.270616Z",
     "shell.execute_reply.started": "2025-05-06T17:43:07.191099Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "predictions_path = f\"{model_to_unlearn_name}_predictions.json\"\n",
    "# classes = ['0 - zero', '1 - one', '2 - two', '3 - three', '4 - four', '5 - five', '6 - six', '7 - seven', '8 - eight', '9 - nine']\n",
    "show_metrics(predictions_path, classes, model_to_unlearn_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning on Shadow Twins\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T17:43:31.327956Z",
     "iopub.status.busy": "2025-05-06T17:43:31.327315Z",
     "iopub.status.idle": "2025-05-06T17:43:31.347627Z",
     "shell.execute_reply": "2025-05-06T17:43:31.346628Z",
     "shell.execute_reply.started": "2025-05-06T17:43:31.327924Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "gold_train_subset = gold_train_loader.dataset\n",
    "\n",
    "fine_tune_results = {}\n",
    "for method_name, twin_mapping, similarity_metrics in twin_mappings_list:\n",
    "    print(f\"\\n=== Fine-tuning using replacement method: {method_name} ===\")\n",
    "\n",
    "    twin_indices = list(twin_mapping.values())\n",
    "    twin_subset = Subset(train_dataset_full, twin_indices)\n",
    "    combined_train_dataset = ConcatDataset([gold_train_subset, twin_subset])\n",
    "    fine_tune_loader = DataLoader(\n",
    "        combined_train_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        num_workers=4,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    ft_model_name = f\"fine_tuned_{model_to_unlearn_name}_on_{method_name}\"\n",
    "    model_ft = copy.deepcopy(unlearned_model)\n",
    "\n",
    "    LR_finetune = 1e-3\n",
    "    # optimizer_ft = optim.AdamW(model_ft.parameters(), lr=LR_finetune, weight_decay=1e-4)\n",
    "    optimizer_ft = optim.Adam(model_ft.parameters(), lr=LR_finetune, weight_decay=0)\n",
    "\n",
    "    start_time = time.perf_counter()\n",
    "    train_model(\n",
    "        model_ft,\n",
    "        ft_model_name,\n",
    "        fine_tune_loader,\n",
    "        gold_valid_loader,\n",
    "        criterion,\n",
    "        optimizer_ft,\n",
    "        num_epochs=1,\n",
    "    )\n",
    "    end_time = time.perf_counter()\n",
    "    training_time = end_time - start_time\n",
    "    print(f\"Training time for {ft_model_name}: {training_time:.6f} seconds\")\n",
    "\n",
    "    # --- Evaluate Fine-Tuned Model ---\n",
    "    model, *_ = init_model_cnn()\n",
    "    test_model(model, ft_model_name, f\"{ft_model_name}_model.pth\", gold_test_loader)\n",
    "\n",
    "    predictions_path = f\"{ft_model_name}_predictions.json\"\n",
    "    show_metrics(predictions_path, classes, ft_model_name)\n",
    "\n",
    "print(f\"\\n=== Fine-tuning using remaining set ===\")\n",
    "ft_model_name = f\"fine_tuned_{model_to_unlearn_name}_on_remaining\"\n",
    "model_ft = copy.deepcopy(unlearned_model)\n",
    "\n",
    "LR_finetune = 1e-3\n",
    "optimizer_ft = optim.Adam(model_ft.parameters(), lr=LR_finetune, weight_decay=0)\n",
    "scheduler_ft = StepLR(optimizer_ft, step_size=2, gamma=0.5)\n",
    "\n",
    "start_time = time.perf_counter()\n",
    "train_model(\n",
    "    model_ft,\n",
    "    ft_model_name,\n",
    "    gold_train_loader,\n",
    "    gold_valid_loader,\n",
    "    criterion,\n",
    "    optimizer_ft,\n",
    "    num_epochs=1,\n",
    ")\n",
    "end_time = time.perf_counter()\n",
    "training_time = end_time - start_time\n",
    "print(f\"Training time for {ft_model_name}: {training_time:.6f} seconds\")\n",
    "\n",
    "\n",
    "# --- Evaluate Fine-Tuned Model ---\n",
    "model, *_ = init_model_cnn()\n",
    "test_model(model, ft_model_name, f\"{ft_model_name}_model.pth\", gold_test_loader)\n",
    "\n",
    "predictions_path = f\"{ft_model_name}_predictions.json\"\n",
    "show_metrics(predictions_path, classes, ft_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T18:16:57.490444Z",
     "iopub.status.busy": "2025-05-06T18:16:57.490176Z",
     "iopub.status.idle": "2025-05-06T18:20:22.310603Z",
     "shell.execute_reply": "2025-05-06T18:20:22.309961Z",
     "shell.execute_reply.started": "2025-05-06T18:16:57.490429Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import copy\n",
    "import json\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from utils.utils import DEVICE\n",
    "\n",
    "\n",
    "def test_model(model, model_name, model_path, test_loader):\n",
    "    \"\"\"\n",
    "    Loads model state, evaluates on the test loader, saves predictions,\n",
    "    and returns the test accuracy.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The model instance (architecture should match the state dict).\n",
    "        model_name (str): A name for saving predictions and logging.\n",
    "        model_path (str or Path): Path to the saved model state dictionary (.pth file).\n",
    "        test_loader (DataLoader): DataLoader for the test set.\n",
    "\n",
    "    Returns:\n",
    "        float: The calculated accuracy score on the test set.\n",
    "    \"\"\"\n",
    "    print(f\"Loading and testing model: {model_name} from {model_path}\")\n",
    "    try:\n",
    "        # Load state dict into the provided model object\n",
    "        model.load_state_dict(torch.load(model_path, map_location=DEVICE))\n",
    "        model.to(DEVICE)\n",
    "        print(\"Model state loaded successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model state dict from {model_path}: {e}\")\n",
    "        return 0.0\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        for inputs, labels in tqdm(test_loader, desc=f\"Evaluating {model_name}\"):\n",
    "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            predictions.extend(preds.cpu().numpy())\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # Ensure lists are flat if model outputs batches > 1 consistently\n",
    "    predictions = np.array(predictions).flatten()\n",
    "    true_labels = np.array(true_labels).flatten()\n",
    "\n",
    "    # --- Calculate Accuracy ---\n",
    "    if len(true_labels) > 0:\n",
    "        accuracy = accuracy_score(true_labels, predictions)\n",
    "        print(f\"Test Accuracy for {model_name}: {accuracy:.4f}\")\n",
    "    else:\n",
    "        print(\"Warning: Test loader was empty. Accuracy is 0.\")\n",
    "        accuracy = 0.0\n",
    "\n",
    "    results = {\"predictions\": predictions.tolist(), \"true_labels\": true_labels.tolist()}\n",
    "    pred_file = f\"{model_name}_predictions.json\"\n",
    "    try:\n",
    "        with open(pred_file, \"w\") as f:\n",
    "            json.dump(results, f)\n",
    "        print(f\"Predictions and labels saved to {pred_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving predictions to {pred_file}: {e}\")\n",
    "\n",
    "    # --- Return Accuracy ---\n",
    "    return float(accuracy)\n",
    "\n",
    "\n",
    "RUN_DIR = Path(\"./unlearning_runs\")\n",
    "RUN_DIR.mkdir(exist_ok=True)\n",
    "NUM_RUNS = 5\n",
    "gold_standard_accuracy = 0.9887\n",
    "print(f\"Using Gold Standard Accuracy: {gold_standard_accuracy:.4f}\")\n",
    "\n",
    "\n",
    "# %%\n",
    "# =============================================================================\n",
    "# Phase 1: Run Approximate Unlearning Multiple Times\n",
    "# =============================================================================\n",
    "\n",
    "import time\n",
    "\n",
    "fisher_accuracies = []\n",
    "fisher_model_paths = []\n",
    "fisher_times = []\n",
    "influence_accuracies = []\n",
    "influence_model_paths = []\n",
    "influence_times = []\n",
    "\n",
    "# --- Fisher Runs ---\n",
    "print(f\"\\n--- Running Fisher Unlearning ({NUM_RUNS} runs) ---\")\n",
    "for i in range(NUM_RUNS):\n",
    "    run_name = f\"fisher_run_{i+1}\"\n",
    "    print(f\"  Fisher Run {i+1}/{NUM_RUNS} ({run_name})\")\n",
    "\n",
    "    # Load fresh model instance and state dict for each run\n",
    "    model_to_unlearn, *_ = init_model_cnn()\n",
    "    model_to_unlearn = copy.deepcopy(original_model)\n",
    "    model_to_unlearn.to(DEVICE)\n",
    "\n",
    "    start_time = time.perf_counter()\n",
    "    # Perform unlearning\n",
    "    unlearned_model_fisher = iterative_fisher_unlearn_simplified_golatkar(\n",
    "        model=model_to_unlearn,\n",
    "        criterion=criterion,\n",
    "        full_dataset=rep_subset,\n",
    "        removal_indices=unlearn_relative_indices,\n",
    "        sigma_lambda_term=35e-4,\n",
    "        deletion_batch_size=25,\n",
    "        compute_batch_size=512,\n",
    "        eps=1e-6,\n",
    "    )\n",
    "    end_time = time.perf_counter()\n",
    "    duration = end_time - start_time\n",
    "    fisher_times.append(duration)\n",
    "    print(f\"  Fisher Run {i+1} Time: {duration:.2f} seconds\")\n",
    "    model_path = RUN_DIR / f\"{run_name}.pth\"\n",
    "    save_model(unlearned_model_fisher, model_path)\n",
    "\n",
    "    # Test the unlearned model\n",
    "\n",
    "    eval_model, *_ = init_model_cnn()\n",
    "    accuracy = test_model(eval_model, run_name, model_path, gold_test_loader)\n",
    "    fisher_accuracies.append(accuracy)\n",
    "    fisher_model_paths.append(model_path)\n",
    "    print(f\"  Fisher Run {i+1} Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    # Cleanup\n",
    "    del model_to_unlearn, unlearned_model_fisher, eval_model\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "# --- Influence Runs ---\n",
    "print(f\"\\n--- Running Influence Unlearning ({NUM_RUNS} runs) ---\")\n",
    "for i in range(NUM_RUNS):\n",
    "    run_name = f\"influence_run_{i+1}\"\n",
    "    print(f\"  Influence Run {i+1}/{NUM_RUNS} ({run_name})\")\n",
    "\n",
    "    # Load fresh model instance and state dict\n",
    "    model_to_unlearn, *_ = init_model_cnn()\n",
    "    model_to_unlearn = copy.deepcopy(original_model)\n",
    "    model_to_unlearn.to(DEVICE)\n",
    "\n",
    "    start_time = time.perf_counter()\n",
    "\n",
    "    # Perform unlearning\n",
    "    unlearned_model_influence = iterative_influence_unlearn(\n",
    "        model=model_to_unlearn,\n",
    "        criterion=criterion,\n",
    "        full_dataset=rep_subset,\n",
    "        removal_indices=unlearn_relative_indices,\n",
    "        deletion_batch_size=25,\n",
    "        compute_batch_size=4,\n",
    "        eps=1e-6,\n",
    "        max_norm=None,\n",
    "        scale=1000,\n",
    "        lissa_depth=5000,\n",
    "    )\n",
    "\n",
    "    end_time = time.perf_counter()\n",
    "    duration = end_time - start_time\n",
    "    influence_times.append(duration)\n",
    "\n",
    "    print(f\"  Fisher Run {i+1} Time: {duration:.2f} seconds\")\n",
    "    model_path = RUN_DIR / f\"{run_name}.pth\"\n",
    "    save_model(unlearned_model_influence, model_path)\n",
    "\n",
    "    # Test\n",
    "    eval_model, *_ = init_model_cnn()\n",
    "    accuracy = test_model(eval_model, run_name, model_path, gold_test_loader)\n",
    "    influence_accuracies.append(accuracy)\n",
    "    influence_model_paths.append(model_path)\n",
    "    print(f\"  Influence Run {i+1} Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    # Cleanup\n",
    "    del model_to_unlearn, unlearned_model_influence, eval_model\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "# %%\n",
    "# =============================================================================\n",
    "# Phase 2: Analyze Unlearning Variability & Select Representative Models\n",
    "# =============================================================================\n",
    "\n",
    "fisher_accuracies = np.array(fisher_accuracies)\n",
    "influence_accuracies = np.array(influence_accuracies)\n",
    "fisher_times = np.array(fisher_times)\n",
    "influence_times = np.array(influence_times)\n",
    "\n",
    "fisher_mean_acc = np.mean(fisher_accuracies)\n",
    "fisher_std_acc = np.std(fisher_accuracies)\n",
    "fisher_var_acc = np.var(fisher_accuracies)\n",
    "fisher_mean_time = np.mean(fisher_times)\n",
    "\n",
    "influence_mean_acc = np.mean(influence_accuracies)\n",
    "influence_std_acc = np.std(influence_accuracies)\n",
    "influence_var_acc = np.var(influence_accuracies)\n",
    "influence_mean_time = np.mean(influence_times)\n",
    "\n",
    "print(\"\\n--- Unlearning Method Stats ---\")\n",
    "print(\n",
    "    f\"Fisher   | Mean Acc: {fisher_mean_acc:.4f}, StdDev Acc: {fisher_std_acc:.4f}, Var Acc: {fisher_var_acc:.4f} | Mean Time: {fisher_mean_time:.2f}s\"\n",
    ")\n",
    "print(\n",
    "    f\"Influence| Mean Acc: {influence_mean_acc:.4f}, StdDev Acc: {influence_std_acc:.4f}, Var Acc: {influence_var_acc:.4f} | Mean Time: {influence_mean_time:.2f}s\"\n",
    ")\n",
    "\n",
    "fisher_median_idx = np.argmin(np.abs(fisher_accuracies - fisher_mean_acc))\n",
    "influence_median_idx = np.argmin(np.abs(influence_accuracies - influence_mean_acc))\n",
    "\n",
    "fisher_rep_model_path = fisher_model_paths[fisher_median_idx]\n",
    "influence_rep_model_path = influence_model_paths[influence_median_idx]\n",
    "\n",
    "fisher_rep_acc = fisher_accuracies[fisher_median_idx]\n",
    "influence_rep_acc = influence_accuracies[influence_median_idx]\n",
    "\n",
    "print(\n",
    "    f\"\\nSelected Representative Fisher Model: {fisher_rep_model_path.name} (Acc: {fisher_rep_acc:.4f})\"\n",
    ")\n",
    "print(\n",
    "    f\"Selected Representative Influence Model: {influence_rep_model_path.name} (Acc: {influence_rep_acc:.4f})\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T18:43:54.099537Z",
     "iopub.status.busy": "2025-05-06T18:43:54.098785Z",
     "iopub.status.idle": "2025-05-06T18:48:06.269791Z",
     "shell.execute_reply": "2025-05-06T18:48:06.269027Z",
     "shell.execute_reply.started": "2025-05-06T18:43:54.099512Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define Epoch settings for healing\n",
    "HEAL_LR = LEARNING_RATE\n",
    "HEAL_BATCH_SIZE = BATCH_SIZE\n",
    "HEAL_EPOCHS_SHORT = 1\n",
    "HEAL_EPOCHS_LONG = 5\n",
    "epoch_settings_to_run = [HEAL_EPOCHS_SHORT, HEAL_EPOCHS_LONG]\n",
    "\n",
    "results = {\"fisher_start\": {}, \"influence_start\": {}}\n",
    "\n",
    "# Define the starting models\n",
    "start_points = {\n",
    "    \"fisher_start\": fisher_rep_model_path,\n",
    "    \"influence_start\": influence_rep_model_path,\n",
    "}\n",
    "\n",
    "# --- Define the 5 Healing Data Loaders/Configurations ---\n",
    "healing_data_configs = []\n",
    "gold_train_subset = gold_train_loader.dataset\n",
    "# Configs 1-4: FT-RemainPlusTwins\n",
    "if \"twin_mappings_list\" in locals() and twin_mappings_list:\n",
    "    print(\"--- Defining Remain+Twins Healing Loaders ---\")\n",
    "    for twin_method_name, twin_mapping, _ in twin_mappings_list:\n",
    "        print(f\"  Processing twin method: {twin_method_name}\")\n",
    "        try:\n",
    "            if not isinstance(twin_mapping, dict) or not twin_mapping:\n",
    "                print(f\"    Skipping {twin_method_name}: invalid mapping.\")\n",
    "                continue\n",
    "            twin_indices = list(twin_mapping.values())\n",
    "            if twin_indices:\n",
    "                twin_subset = Subset(train_dataset_full, twin_indices)\n",
    "                if not isinstance(gold_train_subset, torch.utils.data.Dataset):\n",
    "                    print(\"    Error: gold_train_subset is not a Dataset object.\")\n",
    "                    continue\n",
    "                combined_train_dataset = ConcatDataset([gold_train_subset, twin_subset])\n",
    "                heal_loader = DataLoader(\n",
    "                    combined_train_dataset,\n",
    "                    batch_size=HEAL_BATCH_SIZE,\n",
    "                    shuffle=True,\n",
    "                    num_workers=4,\n",
    "                    pin_memory=True,\n",
    "                )\n",
    "                healing_data_configs.append(\n",
    "                    {\n",
    "                        \"name\": f\"RemainPlusTwins-{twin_method_name}\",\n",
    "                        \"loader\": heal_loader,\n",
    "                    }\n",
    "                )\n",
    "                print(f\"    Loader created for {twin_method_name}.\")\n",
    "            else:\n",
    "                print(f\"    Skipping {twin_method_name}: No twin indices.\")\n",
    "        except Exception as e:\n",
    "            print(f\"    Error creating loader for {twin_method_name}: {e}\")\n",
    "else:\n",
    "    print(\n",
    "        \"Warning: twin_mappings_list not found or empty. Cannot create twin-based healing loaders.\"\n",
    "    )\n",
    "\n",
    "# # Config 5: FT-RemainOnly\n",
    "print(f\"Defined {len(healing_data_configs)} healing data configurations.\")\n",
    "\n",
    "# --- Iterate through starting models, healing datasets, and epoch settings ---\n",
    "for start_point_name, start_model_path in start_points.items():\n",
    "    print(\n",
    "        f\"\\n\\n{'='*20} Processing Start Point: {start_point_name} ({start_model_path.name}) {'='*20}\"\n",
    "    )\n",
    "\n",
    "    # Load the representative model state dict ONCE for this starting point\n",
    "    start_model_state_dict = torch.load(start_model_path, map_location=\"cpu\")\n",
    "    initial_accuracy = (\n",
    "        fisher_rep_acc if start_point_name == \"fisher_start\" else influence_rep_acc\n",
    "    )\n",
    "    print(f\"--- Initial Accuracy for {start_point_name}: {initial_accuracy:.4f} ---\")\n",
    "\n",
    "    # Initialize nested dictionary for this start point\n",
    "    results[start_point_name] = {config[\"name\"]: {} for config in healing_data_configs}\n",
    "\n",
    "    # Iterate through the 5 healing data configurations\n",
    "    for config in healing_data_configs:\n",
    "        healing_config_name = config[\"name\"]\n",
    "        healing_loader = config[\"loader\"]\n",
    "        print(f\"\\n  --- Applying Healing using Data: {healing_config_name} ---\")\n",
    "\n",
    "        # Iterate through the epoch settings (1 and 5 epochs)\n",
    "        for num_epochs in epoch_settings_to_run:\n",
    "            print(f\"    --- Healing for {num_epochs} epoch(s) ---\")\n",
    "            try:\n",
    "                # Load fresh model instance and the STARTING state dict EVERY time\n",
    "                model_to_heal, *_ = init_model_cnn()\n",
    "                model_to_heal.load_state_dict(copy.deepcopy(start_model_state_dict))\n",
    "                model_to_heal.to(DEVICE)\n",
    "\n",
    "                current_lr = HEAL_LR\n",
    "                optimizer_heal = optim.Adam(\n",
    "                    model_to_heal.parameters(), lr=current_lr, weight_decay=0\n",
    "                )\n",
    "\n",
    "                # Define healed model name including epoch count\n",
    "                healed_model_name = f\"healed_{start_point_name}_using_{healing_config_name}_{num_epochs}ep\"\n",
    "\n",
    "                # Run Healing (Fine-tuning)\n",
    "                start_heal_time = time.perf_counter()\n",
    "                train_model(\n",
    "                    model=model_to_heal,\n",
    "                    model_name=healed_model_name,\n",
    "                    train_loader=healing_loader,\n",
    "                    val_loader=gold_valid_loader,\n",
    "                    criterion=criterion,\n",
    "                    optimizer=optimizer_heal,\n",
    "                    num_epochs=num_epochs,\n",
    "                )\n",
    "                end_heal_time = time.perf_counter()\n",
    "                print(f\"      Healing time: {end_heal_time - start_heal_time:.2f}s\")\n",
    "\n",
    "                # Save the healed model\n",
    "                healed_model_path = RUN_DIR / f\"{healed_model_name}.pth\"\n",
    "                save_model(model_to_heal, healed_model_path)\n",
    "\n",
    "                # Test the healed model\n",
    "                eval_model, *_ = init_model_cnn()\n",
    "                accuracy = test_model(\n",
    "                    eval_model, healed_model_name, healed_model_path, gold_test_loader\n",
    "                )\n",
    "                results[start_point_name][healing_config_name][num_epochs] = accuracy\n",
    "                print(f\"      Healed Accuracy ({num_epochs}ep): {accuracy:.4f}\")\n",
    "\n",
    "                # Cleanup\n",
    "                del model_to_heal, optimizer_heal, eval_model\n",
    "                torch.cuda.empty_cache()\n",
    "                gc.collect()\n",
    "\n",
    "            except Exception as e:\n",
    "                print(\n",
    "                    f\"      ERROR during healing ({healing_config_name}, {num_epochs}ep): {e}\"\n",
    "                )\n",
    "                if \"model_to_heal\" in locals():\n",
    "                    del model_to_heal\n",
    "                if \"optimizer_heal\" in locals():\n",
    "                    del optimizer_heal\n",
    "                if \"eval_model\" in locals():\n",
    "                    del eval_model\n",
    "                torch.cuda.empty_cache()\n",
    "                gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T18:48:13.932453Z",
     "iopub.status.busy": "2025-05-06T18:48:13.931781Z",
     "iopub.status.idle": "2025-05-06T18:48:13.940393Z",
     "shell.execute_reply": "2025-05-06T18:48:13.939602Z",
     "shell.execute_reply.started": "2025-05-06T18:48:13.932425Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"\\n\\n--- FINAL RESULTS SUMMARY ---\")\n",
    "gold_standard_accuracy = 0.9872\n",
    "print(f\"Gold Standard Accuracy: {gold_standard_accuracy:.4f}\")\n",
    "\n",
    "epoch_settings_analyzed = [HEAL_EPOCHS_SHORT, HEAL_EPOCHS_LONG]  # e.g., [1, 5]\n",
    "\n",
    "for start_point_name, start_acc in [\n",
    "    (\"fisher_start\", fisher_rep_acc),\n",
    "    (\"influence_start\", influence_rep_acc),\n",
    "]:\n",
    "    print(f\"\\n\\n{'='*10} Healing Summary starting from: {start_point_name} {'='*10}\")\n",
    "    print(f\"  Initial Accuracy: {start_acc:.4f}\")\n",
    "\n",
    "    healing_results_for_start = results.get(start_point_name, {})\n",
    "    if not healing_results_for_start:\n",
    "        print(\"  No healing results found for this starting point.\")\n",
    "        continue\n",
    "\n",
    "    # Analyze results separately for each epoch setting\n",
    "    for num_epochs in epoch_settings_analyzed:\n",
    "        print(f\"\\n  --- Analysis for {num_epochs}-Epoch Healing ---\")\n",
    "\n",
    "        epoch_specific_accs = {}\n",
    "        # Extract accuracies only for the current number of epochs\n",
    "        for config_name, epoch_results in healing_results_for_start.items():\n",
    "            if num_epochs in epoch_results:\n",
    "                # Store accuracy with the config name as the key\n",
    "                epoch_specific_accs[config_name] = epoch_results[num_epochs]\n",
    "\n",
    "        if not epoch_specific_accs:\n",
    "            print(f\"    No results found for {num_epochs}-epoch healing.\")\n",
    "            continue\n",
    "\n",
    "        # Find best method/accuracy for this epoch count\n",
    "        best_method_this_epoch = max(epoch_specific_accs, key=epoch_specific_accs.get)\n",
    "        best_acc_this_epoch = epoch_specific_accs[best_method_this_epoch]\n",
    "        pp_diff_best = (best_acc_this_epoch - gold_standard_accuracy) * 100\n",
    "\n",
    "        # Find worst method/accuracy for this epoch count\n",
    "        worst_method_this_epoch = min(epoch_specific_accs, key=epoch_specific_accs.get)\n",
    "        worst_acc_this_epoch = epoch_specific_accs[worst_method_this_epoch]\n",
    "        pp_diff_worst = (worst_acc_this_epoch - gold_standard_accuracy) * 100\n",
    "\n",
    "        print(\n",
    "            f\"    Best  Accuracy ({num_epochs}ep): {best_acc_this_epoch:.4f} (using {best_method_this_epoch}) | Diff to Gold: {pp_diff_best:+.2f} pp\"\n",
    "        )\n",
    "        print(\n",
    "            f\"    Worst Accuracy ({num_epochs}ep): {worst_acc_this_epoch:.4f}                            | Diff to Gold: {pp_diff_worst:+.2f} pp\"\n",
    "        )\n",
    "\n",
    "print(\"\\n--- End of Summary ---\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
