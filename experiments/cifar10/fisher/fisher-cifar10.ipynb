{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependecies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T17:17:40.109064Z",
     "iopub.status.busy": "2025-04-03T17:17:40.108458Z",
     "iopub.status.idle": "2025-04-03T17:17:40.118512Z",
     "shell.execute_reply": "2025-04-03T17:17:40.117651Z",
     "shell.execute_reply.started": "2025-04-03T17:17:40.109033Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "import copy\n",
    "from torchvision import datasets\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T17:17:40.121413Z",
     "iopub.status.busy": "2025-04-03T17:17:40.120722Z",
     "iopub.status.idle": "2025-04-03T17:17:40.130151Z",
     "shell.execute_reply": "2025-04-03T17:17:40.129373Z",
     "shell.execute_reply.started": "2025-04-03T17:17:40.121362Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "project_root = Path.cwd().resolve().parents[2]\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.append(str(project_root))\n",
    "\n",
    "data_root = project_root / \"data\"\n",
    "data_root.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "from notebook_setup import setup_notebook\n",
    "\n",
    "setup_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T17:17:40.131176Z",
     "iopub.status.busy": "2025-04-03T17:17:40.130953Z",
     "iopub.status.idle": "2025-04-03T17:17:40.151018Z",
     "shell.execute_reply": "2025-04-03T17:17:40.150179Z",
     "shell.execute_reply.started": "2025-04-03T17:17:40.131153Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "from utils.utils import DEVICE\n",
    "\n",
    "print(f\"Device used: {DEVICE}\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "from utils.utils import set_seed\n",
    "\n",
    "set_seed()\n",
    "\n",
    "from utils.utils import save_model\n",
    "\n",
    "from models.resnet50 import load_model_resnet50, init_model_resnet50\n",
    "\n",
    "# Merics\n",
    "from utils.train_test_metrics import test_model, show_metrics\n",
    "\n",
    "# Recreate Dataloaders from json files\n",
    "from methods.naive.naive_utils import recreate_dataloaders\n",
    "\n",
    "# Fisher Information Matrix (FIM) calc and unlearning with FIM\n",
    "from methods.fisher.fisher_utils import (\n",
    "    iterative_fisher_unlearn,\n",
    "    create_unlearning_dataloader,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters (arbitrary chosen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T17:17:40.152518Z",
     "iopub.status.busy": "2025-04-03T17:17:40.152202Z",
     "iopub.status.idle": "2025-04-03T17:17:40.162532Z",
     "shell.execute_reply": "2025-04-03T17:17:40.161778Z",
     "shell.execute_reply.started": "2025-04-03T17:17:40.152482Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256\n",
    "MINI_BATCH_SIZE = 8192\n",
    "\n",
    "SIGMA = 0.1\n",
    "\n",
    "EPS = 1e-6\n",
    "MAX_NORM = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CALL FISHER\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T17:17:40.164697Z",
     "iopub.status.busy": "2025-04-03T17:17:40.164254Z",
     "iopub.status.idle": "2025-04-03T17:17:40.172343Z",
     "shell.execute_reply": "2025-04-03T17:17:40.171470Z",
     "shell.execute_reply.started": "2025-04-03T17:17:40.164665Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model_file = project_root / \"experiments/cifar10/naive/ResNet50_CIFAR10_model.pth\"\n",
    "samples_to_unlearn_file = (\n",
    "    project_root / \"experiments/cifar10/naive/cifar10_samples_to_unlearn_30per.json\"\n",
    ")\n",
    "remaining_dataset_file = (\n",
    "    project_root / \"experiments/cifar10/naive/updated_cifar10_data_splits.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Init models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T17:17:40.173938Z",
     "iopub.status.busy": "2025-04-03T17:17:40.173442Z",
     "iopub.status.idle": "2025-04-03T17:17:40.969622Z",
     "shell.execute_reply": "2025-04-03T17:17:40.968599Z",
     "shell.execute_reply.started": "2025-04-03T17:17:40.173901Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "original_model, original_model_name, criterion, _optimizer, transform = (\n",
    "    load_model_resnet50(model_pth_path=model_file)\n",
    ")\n",
    "\n",
    "model_to_unlearn = copy.deepcopy(original_model)\n",
    "import torch.nn as nn\n",
    "\n",
    "model_to_unlearn = nn.DataParallel(model_to_unlearn, device_ids=[0, 1])\n",
    "model_to_unlearn_name = \"fisher_\" + original_model_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T17:17:40.971203Z",
     "iopub.status.busy": "2025-04-03T17:17:40.970914Z",
     "iopub.status.idle": "2025-04-03T17:17:42.488003Z",
     "shell.execute_reply": "2025-04-03T17:17:42.487284Z",
     "shell.execute_reply.started": "2025-04-03T17:17:40.971172Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_dataset = datasets.CIFAR10(\n",
    "    root=data_root, train=True, transform=transform, download=True\n",
    ")\n",
    "test_dataset = datasets.CIFAR10(\n",
    "    root=data_root, train=False, transform=transform, download=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T17:17:42.489452Z",
     "iopub.status.busy": "2025-04-03T17:17:42.489048Z",
     "iopub.status.idle": "2025-04-03T17:17:42.507449Z",
     "shell.execute_reply": "2025-04-03T17:17:42.506803Z",
     "shell.execute_reply.started": "2025-04-03T17:17:42.489390Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "unlearn_indices, _unlearn_loader = create_unlearning_dataloader(\n",
    "    samples_to_unlearn_file, train_dataset, batch_size=MINI_BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T17:17:42.508603Z",
     "iopub.status.busy": "2025-04-03T17:17:42.508350Z",
     "iopub.status.idle": "2025-04-03T17:17:42.512385Z",
     "shell.execute_reply": "2025-04-03T17:17:42.511580Z",
     "shell.execute_reply.started": "2025-04-03T17:17:42.508579Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# train_loader, _val_loader, test_loader, classes = recreate_dataloaders(\n",
    "#     data_splits_file=remaining_dataset_file,\n",
    "#     datasets=(train_dataset, test_dataset),\n",
    "#     batch_size=BATCH_SIZE)\n",
    "\n",
    "# unlearn_loader = create_unlearning_dataloader(samples_to_unlearn_file, train_dataset, batch_size = MINI_BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FIM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T17:17:42.515583Z",
     "iopub.status.busy": "2025-04-03T17:17:42.515196Z",
     "iopub.status.idle": "2025-04-03T17:17:42.537644Z",
     "shell.execute_reply": "2025-04-03T17:17:42.536793Z",
     "shell.execute_reply.started": "2025-04-03T17:17:42.515557Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import json\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "from utils.utils import DEVICE\n",
    "\n",
    "\n",
    "def compute_gradient_on_subset(model, criterion, dataset_subset, batch_size):\n",
    "    \"\"\"\n",
    "    Compute the average gradient Δ_rem = ∇L(θ, D') over the given dataset_subset.\n",
    "    \"\"\"\n",
    "    dataloader = DataLoader(dataset_subset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    grad_dict = {}\n",
    "    total_samples = 0\n",
    "\n",
    "    model.train()\n",
    "    for inputs, targets in tqdm(dataloader, desc=\"Computing gradients\"):\n",
    "        inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
    "        model.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        current_batch = inputs.size(0)\n",
    "        total_samples += current_batch\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.requires_grad and param.grad is not None:\n",
    "                if name not in grad_dict:\n",
    "                    grad_dict[name] = param.grad.detach().clone() * current_batch\n",
    "                else:\n",
    "                    grad_dict[name] += param.grad.detach() * current_batch\n",
    "\n",
    "    # Average gradients over the entire subset\n",
    "    for name in grad_dict:\n",
    "        grad_dict[name] /= total_samples\n",
    "\n",
    "    return grad_dict\n",
    "\n",
    "\n",
    "def compute_fisher_on_subset(model, criterion, dataset_subset, batch_size):\n",
    "    \"\"\"\n",
    "    Compute a diagonal approximation of the Fisher Information Matrix F over the given dataset_subset.\n",
    "    It averages the squared gradients.\n",
    "    \"\"\"\n",
    "    dataloader = DataLoader(dataset_subset, batch_size=batch_size, shuffle=False)\n",
    "    fisher_diag = {}\n",
    "    total_samples = 0\n",
    "\n",
    "    model.eval()\n",
    "    for inputs, targets in tqdm(dataloader, desc=\"Computing Fisher\"):\n",
    "        inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
    "        model.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        current_batch = inputs.size(0)\n",
    "        total_samples += current_batch\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.requires_grad and param.grad is not None:\n",
    "                if name not in fisher_diag:\n",
    "                    fisher_diag[name] = (param.grad.detach() ** 2) * current_batch\n",
    "                else:\n",
    "                    fisher_diag[name] += (param.grad.detach() ** 2) * current_batch\n",
    "\n",
    "    for name in fisher_diag:\n",
    "        fisher_diag[name] /= total_samples\n",
    "\n",
    "    return fisher_diag\n",
    "\n",
    "\n",
    "def remove_from_fisher_incrementally(\n",
    "    fisher_diag, model, criterion, dataset_removed, batch_size\n",
    "):\n",
    "    dataloader = DataLoader(dataset_removed, batch_size=batch_size, shuffle=False)\n",
    "    total_removed_samples = 0\n",
    "\n",
    "    model.eval()\n",
    "    for inputs, targets in dataloader:\n",
    "        inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
    "        model.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        batch_samples = inputs.size(0)\n",
    "        total_removed_samples += batch_samples\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.requires_grad and param.grad is not None:\n",
    "                fisher_diag[name] -= (param.grad.detach() ** 2) * batch_samples\n",
    "\n",
    "    total_samples_remaining = fisher_diag[\"_total_samples\"] - total_removed_samples\n",
    "    for name in fisher_diag:\n",
    "        if name != \"_total_samples\":\n",
    "            fisher_diag[name] = torch.clamp(fisher_diag[name], min=1e-8)\n",
    "            fisher_diag[name] /= total_samples_remaining\n",
    "    fisher_diag[\"_total_samples\"] = total_samples_remaining\n",
    "\n",
    "    return fisher_diag\n",
    "\n",
    "\n",
    "def iterative_fisher_unlearn(\n",
    "    model,\n",
    "    criterion,\n",
    "    full_dataset,\n",
    "    removal_indices,\n",
    "    sigma,\n",
    "    deletion_batch_size,\n",
    "    compute_batch_size,\n",
    "    eps,\n",
    "    max_norm,\n",
    "):\n",
    "    \"\"\"\n",
    "    Implements the iterative Fisher unlearning procedure following theory:\n",
    "\n",
    "    Inputs:\n",
    "      - model: a pretrained PyTorch model (trained on full dataset D).\n",
    "      - criterion: loss function (e.g., CrossEntropyLoss).\n",
    "      - full_dataset: the full training dataset D (e.g., MNIST training set).\n",
    "      - removal_indices: list of indices (from the JSON file) to be deleted (Dₘ). E.g., 6000 samples.\n",
    "      - sigma: noise parameter σ.\n",
    "      - deletion_batch_size: desired mini-batch size for deletion (m′). E.g., 1000.\n",
    "      - compute_batch_size: batch size used when computing gradients/Fisher (BATCH_SIZE).\n",
    "      - eps: for numerical stability\n",
    "\n",
    "    Procedure:\n",
    "      1. Let current_indices = set(range(len(full_dataset))).\n",
    "      2. Partition removal_indices into mini-batches of size deletion_batch_size.\n",
    "      3. For each mini-batch, update current_indices by removing those indices.\n",
    "      4. Create a Subset from full_dataset using current_indices (this is D').\n",
    "      5. Compute Δ_rem and diagonal Fisher F on D' and update model:\n",
    "             θ ← θ − F⁻¹ Δ_rem + σ · F^(–1/4) · ε.\n",
    "    \"\"\"\n",
    "    full_size = len(full_dataset)\n",
    "    current_indices = set(range(full_size))\n",
    "\n",
    "    # Partition removal_indices into mini-batches, where s = m /m'\n",
    "    removal_list = list(removal_indices)\n",
    "    num_batches = math.ceil(len(removal_list) / deletion_batch_size)\n",
    "    partitioned_removals = [\n",
    "        removal_list[i * deletion_batch_size : (i + 1) * deletion_batch_size]\n",
    "        for i in range(num_batches)\n",
    "    ]\n",
    "    print(\n",
    "        f\"Total deletion samples: {len(removal_list)}; partitioned into {num_batches} mini-batches (each up to {deletion_batch_size} samples).\"\n",
    "    )\n",
    "\n",
    "    # Iterate over each deletion mini-batch\n",
    "    for i, batch in enumerate(\n",
    "        tqdm(partitioned_removals, desc=\"Fisher step over mini-batches\")\n",
    "    ):\n",
    "        # Remove the current batch of indices from current_indices\n",
    "        current_indices -= set(batch)\n",
    "        updated_indices = sorted(list(current_indices))\n",
    "        # Create a Subset corresponding to the updated dataset D' = D \\ (deleted so far)\n",
    "        dataset_remaining = Subset(full_dataset, updated_indices)\n",
    "        print(\n",
    "            f\"Iteration {i+1}/{num_batches}: Remaining dataset size = {len(dataset_remaining)}\"\n",
    "        )\n",
    "\n",
    "        # Compute the average gradient and diagonal Fisher on D'\n",
    "        grad_dict = compute_gradient_on_subset(\n",
    "            model, criterion, dataset_remaining, compute_batch_size\n",
    "        )\n",
    "        fisher_diag = compute_fisher_on_subset(\n",
    "            model, criterion, dataset_remaining, compute_batch_size\n",
    "        )\n",
    "        # Update model parameters using the Newton correction and noise injection\n",
    "        with torch.no_grad():\n",
    "            for name in grad_dict:\n",
    "                grad = grad_dict[name]\n",
    "                norm = grad.norm(2).item()\n",
    "                grad_min = grad.min().item()\n",
    "                grad_max = grad.max().item()\n",
    "                grad_mean = grad.mean().item()\n",
    "                grad_std = grad.std().item()\n",
    "                print(\n",
    "                    f\"[Raw] Param {name}: norm = {norm:.4e}, min = {grad_min:.4e}, max = {grad_max:.4e}, mean = {grad_mean:.4e}, std = {grad_std:.4e}\"\n",
    "                )\n",
    "\n",
    "            # First, compute and clip gradients, and monitor norms\n",
    "            total_grad_norm_before = 0.0\n",
    "            total_grad_norm_after = 0.0\n",
    "            for name in grad_dict:\n",
    "                norm_before = grad_dict[name].norm(2)\n",
    "                total_grad_norm_before += norm_before.item()\n",
    "                if norm_before > max_norm:\n",
    "                    grad_dict[name] = grad_dict[name] * (max_norm / norm_before)\n",
    "                norm_after = grad_dict[name].norm(2)\n",
    "                total_grad_norm_after += norm_after.item()\n",
    "\n",
    "            print(\n",
    "                f\"Iteration {i+1}: Total gradient norm before clipping = {total_grad_norm_before:.4f}\"\n",
    "            )\n",
    "            print(\n",
    "                f\"Iteration {i+1}: Total gradient norm after clipping  = {total_grad_norm_after:.4f}\"\n",
    "            )\n",
    "\n",
    "            # Now, update model parameters using the clipped gradients and monitor the Newton update norm\n",
    "            total_update_norm = 0.0\n",
    "            for name, param in model.named_parameters():\n",
    "                if param.requires_grad:\n",
    "                    inv_fisher = (fisher_diag[name] + eps).pow(-1)\n",
    "                    newton_update = inv_fisher * grad_dict[name]\n",
    "                    total_update_norm += newton_update.norm(2).item()\n",
    "                    param.data = param.data - newton_update\n",
    "\n",
    "                    inv_fisher_quarter = (fisher_diag[name] + eps).pow(-0.25)\n",
    "                    noise = torch.randn_like(param.data)\n",
    "                    param.data = param.data + sigma * inv_fisher_quarter * noise\n",
    "\n",
    "            print(\n",
    "                f\"Iteration {i+1}: Total Newton update norm = {total_update_norm:.4f}\"\n",
    "            )\n",
    "        print(f\"Iteration {i+1}/{num_batches} update completed.\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unlearn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T17:17:42.538832Z",
     "iopub.status.busy": "2025-04-03T17:17:42.538557Z",
     "iopub.status.idle": "2025-04-03T17:32:20.181258Z",
     "shell.execute_reply": "2025-04-03T17:32:20.180381Z",
     "shell.execute_reply.started": "2025-04-03T17:17:42.538808Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.perf_counter()\n",
    "\n",
    "unlearned_model = iterative_fisher_unlearn(\n",
    "    model_to_unlearn,\n",
    "    criterion,\n",
    "    train_dataset,\n",
    "    unlearn_indices,\n",
    "    SIGMA,\n",
    "    deletion_batch_size=MINI_BATCH_SIZE,\n",
    "    compute_batch_size=BATCH_SIZE,\n",
    "    eps=EPS,\n",
    "    max_norm=MAX_NORM,\n",
    ")\n",
    "\n",
    "end_time = time.perf_counter()  # End timer\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "print(f\"Execution time: {elapsed_time:.6f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T17:32:20.183046Z",
     "iopub.status.busy": "2025-04-03T17:32:20.182596Z",
     "iopub.status.idle": "2025-04-03T17:32:20.324658Z",
     "shell.execute_reply": "2025-04-03T17:32:20.323763Z",
     "shell.execute_reply.started": "2025-04-03T17:32:20.183005Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "save_model(unlearned_model, f\"{model_to_unlearn_name}_model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test and metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T17:32:20.326089Z",
     "iopub.status.busy": "2025-04-03T17:32:20.325844Z",
     "iopub.status.idle": "2025-04-03T17:32:20.330601Z",
     "shell.execute_reply": "2025-04-03T17:32:20.329780Z",
     "shell.execute_reply.started": "2025-04-03T17:32:20.326065Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "classes = test_dataset.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T17:32:20.331833Z",
     "iopub.status.busy": "2025-04-03T17:32:20.331567Z",
     "iopub.status.idle": "2025-04-03T17:32:55.266449Z",
     "shell.execute_reply": "2025-04-03T17:32:55.265463Z",
     "shell.execute_reply.started": "2025-04-03T17:32:20.331809Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model, *_ = init_model_resnet50()\n",
    "import torch.nn as nn\n",
    "\n",
    "model_to_unlearn = nn.DataParallel(model_to_unlearn, device_ids=[0, 1])\n",
    "model_path = f\"{model_to_unlearn_name}_model.pth\"\n",
    "test_model(model, model_to_unlearn_name, model_path, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T17:32:55.268838Z",
     "iopub.status.busy": "2025-04-03T17:32:55.267809Z",
     "iopub.status.idle": "2025-04-03T17:32:55.927890Z",
     "shell.execute_reply": "2025-04-03T17:32:55.927027Z",
     "shell.execute_reply.started": "2025-04-03T17:32:55.268803Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "predictions_path = f\"{model_to_unlearn_name}_predictions.json\"\n",
    "# classes = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "show_metrics(predictions_path, classes, model_to_unlearn_name)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "isSourceIdPinned": true,
     "modelId": 177544,
     "modelInstanceId": 155068,
     "sourceId": 181928,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
